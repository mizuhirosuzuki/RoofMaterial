{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZIjavb0Vb21"
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2771,
     "status": "ok",
     "timestamp": 1576100038996,
     "user": {
      "displayName": "MIZUHIRO SUZUKI",
      "photoUrl": "",
      "userId": "11994014812088599130"
     },
     "user_tz": 360
    },
    "id": "nwbCYEDnVeVW",
    "outputId": "3e360a0a-3ad8-4ea5-8372-73f2028469a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from keras import datasets, layers, models\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback, Callback\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image as krs_image\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Flatten, Input, concatenate, GlobalAveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "import random\n",
    "from glob import glob\n",
    "import tempfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLBnFge9fFaZ"
   },
   "source": [
    "## Generator to load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBjJHoRBL7LP"
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_generator(images_list, dataframe, lb_label, continuous_var_list, categorical_var_list, minmaxscaler, lb_list, batch_size, mode, augment = None):\n",
    "    i = 0\n",
    "    # if not evaluation generator, shuffle the image \n",
    "    if mode == 'train':\n",
    "        random.shuffle(images_list)\n",
    "    while True:        \n",
    "        images = []\n",
    "        csv_continuous_features = []\n",
    "        csv_categorical_features = []\n",
    "        labels = []\n",
    "        \n",
    "        while len(images) < batch_size:\n",
    "            if i == len(images_list):\n",
    "                # if evaluation generator, break the loop when the last image is retrieved\n",
    "                if mode == 'eval':\n",
    "                    break\n",
    "                i = 0\n",
    "                random.shuffle(images_list)                \n",
    "                  \n",
    "            # Read image from list and convert to array\n",
    "            image_path = images_list[i]\n",
    "            image_name = os.path.basename(image_path).replace('.jpg', '')\n",
    "            image = krs_image.load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "            image = np.asarray(image)\n",
    "            images.append(image)\n",
    "\n",
    "            # Read data from csv using the name of current image\n",
    "            csv_row = dataframe[dataframe.id == image_name]\n",
    "            \n",
    "            # extract continuous features\n",
    "            csv_continuous_features.append(np.array(csv_row[continuous_var_list])[0])\n",
    "            \n",
    "            # extract categorical features\n",
    "            csv_categorical_features.append(np.array(csv_row[categorical_var_list])[0])\n",
    "\n",
    "            # # just to check if data are correctly retrieved...\n",
    "            # print(image_name)\n",
    "            \n",
    "            if mode == 'train':\n",
    "                label = np.array(csv_row['label'])[0]\n",
    "                labels.append(label)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        images = np.array(images)\n",
    "        if augment != None:\n",
    "            if mode == 'train':\n",
    "                (images, labels) = next(augment.flow(images, labels, batch_size = batch_size, shuffle = False))\n",
    "            elif mode == 'eval':\n",
    "                images = next(augment.flow(images, batch_size = batch_size, shuffle = False))\n",
    "        elif augment == None:\n",
    "            datagen_rescale = ImageDataGenerator(rescale = 1./255, featurewise_center = True)\n",
    "            datagen_rescale.mean = [train_data.R_mean.mean() / 255, train_data.G_mean.mean() / 255, train_data.B_mean.mean() / 255]\n",
    "            if mode == 'train':\n",
    "                (images, labels) = next(datagen_rescale.flow(images, labels, batch_size = batch_size, shuffle = False))\n",
    "            elif mode == 'eval':\n",
    "                images = next(datagen_rescale.flow(images, batch_size = batch_size, shuffle = False))\n",
    "            \n",
    "        # rescale continuous features\n",
    "        csv_continuous_features = minmaxscaler.transform(np.array(csv_continuous_features))\n",
    "        \n",
    "        # convert categorical features into one-hot encoding\n",
    "        csv_categorical_features_temp = lb_list[0].transform(np.array(csv_categorical_features)[:, 0])\n",
    "        \n",
    "        if len(categorical_var_list) > 0:\n",
    "            for j in range(1, len(categorical_var_list)):\n",
    "                np.hstack(csv_categorical_features_temp, lb_list[j].transform(csv_categorical_features_temp[:, j]))\n",
    "        csv_categorical_features = csv_categorical_features_temp\n",
    "        \n",
    "        csv = np.hstack([csv_continuous_features, csv_categorical_features])\n",
    "        \n",
    "        if mode == 'train':\n",
    "            labels = lb_label.transform(labels)\n",
    "            yield [np.array(images), csv], labels\n",
    "        elif mode == 'eval':\n",
    "            yield [np.array(images), csv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hktipl5GL7ku"
   },
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqUBGgC9h2Yr"
   },
   "source": [
    "### Model for training with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cy_F8orGfHey"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "path = '/content/drive/My Drive/stac/'\n",
    "train_dir = os.path.join(path, 'image_resize/train/')\n",
    "test_dir = os.path.join(path, 'image_resize/test/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6VhUyLQftMU"
   },
   "outputs": [],
   "source": [
    "train_data_csv = pd.read_csv(os.path.join(path, 'image_resize/train/train_data_with_dist.csv'))\n",
    "test_data_csv = pd.read_csv(os.path.join(path, 'image_resize/test/test_data_with_dist.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29570,
     "status": "ok",
     "timestamp": 1576100241763,
     "user": {
      "displayName": "MIZUHIRO SUZUKI",
      "photoUrl": "",
      "userId": "11994014812088599130"
     },
     "user_tz": 360
    },
    "id": "KvAxsdsmYTXE",
    "outputId": "20d52db4-acf2-4ad4-83d6-d30ba89d27cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>centroid_x</th>\n",
       "      <th>centroid_y</th>\n",
       "      <th>log_building_area</th>\n",
       "      <th>building_vertices</th>\n",
       "      <th>building_width</th>\n",
       "      <th>building_height</th>\n",
       "      <th>place</th>\n",
       "      <th>R_mean</th>\n",
       "      <th>G_mean</th>\n",
       "      <th>B_mean</th>\n",
       "      <th>R_std</th>\n",
       "      <th>G_std</th>\n",
       "      <th>B_std</th>\n",
       "      <th>H_mean</th>\n",
       "      <th>S_mean</th>\n",
       "      <th>V_mean</th>\n",
       "      <th>H_std</th>\n",
       "      <th>S_std</th>\n",
       "      <th>V_std</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>distance_10_all</th>\n",
       "      <th>distance_20_all</th>\n",
       "      <th>distance_50_all</th>\n",
       "      <th>distance_10_train</th>\n",
       "      <th>distance_20_train</th>\n",
       "      <th>distance_50_train</th>\n",
       "      <th>concrete_10_train</th>\n",
       "      <th>healthy_10_train</th>\n",
       "      <th>incomplete_10_train</th>\n",
       "      <th>irregular_10_train</th>\n",
       "      <th>other_10_train</th>\n",
       "      <th>concrete_20_train</th>\n",
       "      <th>healthy_20_train</th>\n",
       "      <th>incomplete_20_train</th>\n",
       "      <th>irregular_20_train</th>\n",
       "      <th>other_20_train</th>\n",
       "      <th>concrete_50_train</th>\n",
       "      <th>healthy_50_train</th>\n",
       "      <th>incomplete_50_train</th>\n",
       "      <th>irregular_50_train</th>\n",
       "      <th>other_50_train</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7a3f2a10</td>\n",
       "      <td>concrete_cement</td>\n",
       "      <td>-74.158686</td>\n",
       "      <td>4.555175</td>\n",
       "      <td>-18.936547</td>\n",
       "      <td>5</td>\n",
       "      <td>9.549665</td>\n",
       "      <td>13.229584</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>146.437735</td>\n",
       "      <td>134.216407</td>\n",
       "      <td>252.429891</td>\n",
       "      <td>38.295300</td>\n",
       "      <td>41.715748</td>\n",
       "      <td>23.759252</td>\n",
       "      <td>0.154946</td>\n",
       "      <td>0.130405</td>\n",
       "      <td>0.598747</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>0.112253</td>\n",
       "      <td>0.150610</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>7a3f2a10.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7a1f731e</td>\n",
       "      <td>irregular_metal</td>\n",
       "      <td>-74.158750</td>\n",
       "      <td>4.555195</td>\n",
       "      <td>-18.939354</td>\n",
       "      <td>5</td>\n",
       "      <td>10.043447</td>\n",
       "      <td>12.529615</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>196.166857</td>\n",
       "      <td>191.627553</td>\n",
       "      <td>254.114754</td>\n",
       "      <td>47.644906</td>\n",
       "      <td>56.686724</td>\n",
       "      <td>13.799618</td>\n",
       "      <td>0.400566</td>\n",
       "      <td>0.072351</td>\n",
       "      <td>0.777261</td>\n",
       "      <td>0.181233</td>\n",
       "      <td>0.112066</td>\n",
       "      <td>0.184649</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>7a1f731e.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7a424ad8</td>\n",
       "      <td>healthy_metal</td>\n",
       "      <td>-74.158802</td>\n",
       "      <td>4.555230</td>\n",
       "      <td>-19.628413</td>\n",
       "      <td>5</td>\n",
       "      <td>7.561388</td>\n",
       "      <td>7.970262</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>200.360387</td>\n",
       "      <td>197.030956</td>\n",
       "      <td>252.326710</td>\n",
       "      <td>51.542386</td>\n",
       "      <td>61.830632</td>\n",
       "      <td>23.825935</td>\n",
       "      <td>0.469335</td>\n",
       "      <td>0.075763</td>\n",
       "      <td>0.793850</td>\n",
       "      <td>0.151765</td>\n",
       "      <td>0.116666</td>\n",
       "      <td>0.204333</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>7a424ad8.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7a3edc5e</td>\n",
       "      <td>healthy_metal</td>\n",
       "      <td>-74.158993</td>\n",
       "      <td>4.554755</td>\n",
       "      <td>-18.914724</td>\n",
       "      <td>5</td>\n",
       "      <td>10.414648</td>\n",
       "      <td>12.632232</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>220.817573</td>\n",
       "      <td>215.402966</td>\n",
       "      <td>253.556840</td>\n",
       "      <td>32.110044</td>\n",
       "      <td>36.695000</td>\n",
       "      <td>18.379622</td>\n",
       "      <td>0.366897</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.872029</td>\n",
       "      <td>0.156841</td>\n",
       "      <td>0.043357</td>\n",
       "      <td>0.118184</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>7a3edc5e.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7a303a6e</td>\n",
       "      <td>healthy_metal</td>\n",
       "      <td>-74.158795</td>\n",
       "      <td>4.554937</td>\n",
       "      <td>-19.027574</td>\n",
       "      <td>5</td>\n",
       "      <td>13.587928</td>\n",
       "      <td>10.044892</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>184.202970</td>\n",
       "      <td>174.976720</td>\n",
       "      <td>252.179354</td>\n",
       "      <td>55.540450</td>\n",
       "      <td>64.609198</td>\n",
       "      <td>25.386747</td>\n",
       "      <td>0.337339</td>\n",
       "      <td>0.098513</td>\n",
       "      <td>0.727034</td>\n",
       "      <td>0.144371</td>\n",
       "      <td>0.125795</td>\n",
       "      <td>0.214659</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>7a303a6e.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        id  ... other_50_train      filename\n",
       "0           0  7a3f2a10  ...       0.076923  7a3f2a10.jpg\n",
       "1           1  7a1f731e  ...       0.071429  7a1f731e.jpg\n",
       "2           2  7a424ad8  ...       0.066667  7a424ad8.jpg\n",
       "3           3  7a3edc5e  ...       0.022727  7a3edc5e.jpg\n",
       "4           4  7a303a6e  ...       0.032258  7a303a6e.jpg\n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_csv['filename'] = train_data_csv.id + '.jpg'\n",
    "train_data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29176,
     "status": "ok",
     "timestamp": 1576100241763,
     "user": {
      "displayName": "MIZUHIRO SUZUKI",
      "photoUrl": "",
      "userId": "11994014812088599130"
     },
     "user_tz": 360
    },
    "id": "zqt4dr-6anK5",
    "outputId": "a3f4f5ed-f28c-4b03-c82d-e845463c1174"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>centroid_x</th>\n",
       "      <th>centroid_y</th>\n",
       "      <th>log_building_area</th>\n",
       "      <th>building_vertices</th>\n",
       "      <th>building_width</th>\n",
       "      <th>building_height</th>\n",
       "      <th>place</th>\n",
       "      <th>R_mean</th>\n",
       "      <th>G_mean</th>\n",
       "      <th>B_mean</th>\n",
       "      <th>R_std</th>\n",
       "      <th>G_std</th>\n",
       "      <th>B_std</th>\n",
       "      <th>H_mean</th>\n",
       "      <th>S_mean</th>\n",
       "      <th>V_mean</th>\n",
       "      <th>H_std</th>\n",
       "      <th>S_std</th>\n",
       "      <th>V_std</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>distance_10_all</th>\n",
       "      <th>distance_20_all</th>\n",
       "      <th>distance_50_all</th>\n",
       "      <th>distance_10_train</th>\n",
       "      <th>distance_20_train</th>\n",
       "      <th>distance_50_train</th>\n",
       "      <th>concrete_10_train</th>\n",
       "      <th>healthy_10_train</th>\n",
       "      <th>incomplete_10_train</th>\n",
       "      <th>irregular_10_train</th>\n",
       "      <th>other_10_train</th>\n",
       "      <th>concrete_20_train</th>\n",
       "      <th>healthy_20_train</th>\n",
       "      <th>incomplete_20_train</th>\n",
       "      <th>irregular_20_train</th>\n",
       "      <th>other_20_train</th>\n",
       "      <th>concrete_50_train</th>\n",
       "      <th>healthy_50_train</th>\n",
       "      <th>incomplete_50_train</th>\n",
       "      <th>irregular_50_train</th>\n",
       "      <th>other_50_train</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7a4d630a</td>\n",
       "      <td>-74.158953</td>\n",
       "      <td>4.554654</td>\n",
       "      <td>-19.139941</td>\n",
       "      <td>5</td>\n",
       "      <td>10.039401</td>\n",
       "      <td>9.375537</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>177.755425</td>\n",
       "      <td>178.556806</td>\n",
       "      <td>251.654125</td>\n",
       "      <td>39.328505</td>\n",
       "      <td>43.260665</td>\n",
       "      <td>28.631985</td>\n",
       "      <td>0.437731</td>\n",
       "      <td>0.088424</td>\n",
       "      <td>0.716892</td>\n",
       "      <td>0.181592</td>\n",
       "      <td>0.055217</td>\n",
       "      <td>0.155041</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>7a4d630a.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7a4bbbd6</td>\n",
       "      <td>-74.159509</td>\n",
       "      <td>4.554677</td>\n",
       "      <td>-19.125909</td>\n",
       "      <td>5</td>\n",
       "      <td>8.930837</td>\n",
       "      <td>13.331169</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>177.423525</td>\n",
       "      <td>163.287624</td>\n",
       "      <td>251.069084</td>\n",
       "      <td>39.900712</td>\n",
       "      <td>45.451972</td>\n",
       "      <td>29.278953</td>\n",
       "      <td>0.203984</td>\n",
       "      <td>0.110186</td>\n",
       "      <td>0.709839</td>\n",
       "      <td>0.094666</td>\n",
       "      <td>0.096707</td>\n",
       "      <td>0.147675</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>7a4bbbd6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7a4ac744</td>\n",
       "      <td>-74.158910</td>\n",
       "      <td>4.555028</td>\n",
       "      <td>-19.116754</td>\n",
       "      <td>5</td>\n",
       "      <td>8.372523</td>\n",
       "      <td>12.637685</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>191.145525</td>\n",
       "      <td>180.013555</td>\n",
       "      <td>250.787490</td>\n",
       "      <td>49.441325</td>\n",
       "      <td>59.045194</td>\n",
       "      <td>30.095502</td>\n",
       "      <td>0.280248</td>\n",
       "      <td>0.092685</td>\n",
       "      <td>0.755424</td>\n",
       "      <td>0.115600</td>\n",
       "      <td>0.135139</td>\n",
       "      <td>0.188650</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>7a4ac744.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7a4881fa</td>\n",
       "      <td>-74.158861</td>\n",
       "      <td>4.555007</td>\n",
       "      <td>-19.178307</td>\n",
       "      <td>5</td>\n",
       "      <td>7.844693</td>\n",
       "      <td>12.018867</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>183.273582</td>\n",
       "      <td>175.581329</td>\n",
       "      <td>251.345270</td>\n",
       "      <td>43.336066</td>\n",
       "      <td>45.729721</td>\n",
       "      <td>28.048951</td>\n",
       "      <td>0.203959</td>\n",
       "      <td>0.070619</td>\n",
       "      <td>0.732613</td>\n",
       "      <td>0.131760</td>\n",
       "      <td>0.065752</td>\n",
       "      <td>0.157899</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>7a4881fa.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7a4aa4a8</td>\n",
       "      <td>-74.158777</td>\n",
       "      <td>4.554996</td>\n",
       "      <td>-18.840272</td>\n",
       "      <td>5</td>\n",
       "      <td>14.114200</td>\n",
       "      <td>10.788166</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>186.983237</td>\n",
       "      <td>177.131517</td>\n",
       "      <td>251.791991</td>\n",
       "      <td>47.946749</td>\n",
       "      <td>57.292896</td>\n",
       "      <td>26.487065</td>\n",
       "      <td>0.323969</td>\n",
       "      <td>0.106422</td>\n",
       "      <td>0.745602</td>\n",
       "      <td>0.171864</td>\n",
       "      <td>0.113670</td>\n",
       "      <td>0.182219</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>7a4aa4a8.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        id  ...  other_50_train      filename\n",
       "0           0  7a4d630a  ...        0.024390  7a4d630a.jpg\n",
       "1           1  7a4bbbd6  ...        0.021739  7a4bbbd6.jpg\n",
       "2           2  7a4ac744  ...        0.032258  7a4ac744.jpg\n",
       "3           3  7a4881fa  ...        0.032258  7a4881fa.jpg\n",
       "4           4  7a4aa4a8  ...        0.037037  7a4aa4a8.jpg\n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_csv['filename'] = test_data_csv.id + '.jpg'\n",
    "test_data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XK9rBNX3al5g"
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, train_label, valid_label = train_test_split(train_data_csv, \n",
    "                                                                    train_data_csv.label, \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    stratify = train_data_csv.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nd67ejcVn4oi"
   },
   "outputs": [],
   "source": [
    "total_train = len(train_data)\n",
    "total_valid = len(valid_data)\n",
    "total_test = len(test_data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tr2S6f00iKEY"
   },
   "source": [
    "#### Preparation for loading data with generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oz2CEyM1iIq8"
   },
   "outputs": [],
   "source": [
    "# Specify which variables are used in NN\n",
    "\n",
    "# continuous_var_list = ['centroid_x', 'centroid_y', 'log_building_area', 'building_vertices', 'building_width', 'building_height']\n",
    "continuous_var_list = ['log_building_area', 'building_vertices', 'building_width', 'building_height',\n",
    "                        'R_mean', 'G_mean', 'B_mean', 'R_std', 'G_std', 'B_std',\n",
    "                        'H_mean', 'S_mean', 'V_mean', 'H_std', 'S_std', 'V_std',\n",
    "                        'distance_10_all', 'distance_20_all', 'distance_50_all',\n",
    "                        'concrete_10_train', 'healthy_10_train', 'incomplete_10_train', 'irregular_10_train', 'other_10_train',\n",
    "                        'concrete_20_train', 'healthy_20_train', 'incomplete_20_train', 'irregular_20_train', 'other_20_train',\n",
    "                       'concrete_50_train', 'healthy_50_train', 'incomplete_50_train', 'irregular_50_train', 'other_50_train']\n",
    "categorical_var_list = ['place']\n",
    "\n",
    "# Define a rescaler so that continuous variables are within the range of [0, 1]\n",
    "minmaxscaler = MinMaxScaler()\n",
    "minmaxscaler.fit(train_data_csv[continuous_var_list])\n",
    "\n",
    "# Define one-hot encoders for categorical variables\n",
    "lb0 = LabelBinarizer()\n",
    "lb0.fit(train_data_csv[categorical_var_list[0]])\n",
    "lb_list = [lb0]\n",
    "\n",
    "# Define a one-hot encoder for label\n",
    "lb_label = LabelBinarizer()\n",
    "lb_label.fit(train_data_csv.label)\n",
    "\n",
    "# Create an empty data generator, which will be used in training data generator\n",
    "datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                            horizontal_flip = True,\n",
    "                            vertical_flip = True,\n",
    "                            rotation_range = 45,\n",
    "                            featurewise_center = True)\n",
    "datagen.mean = [train_data.R_mean.mean() / 255, train_data.G_mean.mean() / 255, train_data.B_mean.mean() / 255]\n",
    "\n",
    "datagen_test = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  rotation_range=10.,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                  height_shift_range = 0.1,\n",
    "                                  featurewise_center = True)\n",
    "datagen_test.mean = [train_data.R_mean.mean() / 255, train_data.G_mean.mean() / 255, train_data.B_mean.mean() / 255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJv4SilgjUzK"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the image list and csv\n",
    "path = '/content/drive/My Drive/stac/'\n",
    "train_image_file_list = glob(os.path.join(path, 'image_resize/train/*.jpg'))\n",
    "test_image_file_list = glob(os.path.join(path, 'image_resize/test/*.jpg'))\n",
    "\n",
    "valid_image_file_index = [os.path.basename(train_image_file_list[i]).replace('.jpg','') in list(valid_data.id) for i in range(len(train_image_file_list))]\n",
    "train_image_file_index = [os.path.basename(train_image_file_list[i]).replace('.jpg','') in list(train_data.id) for i in range(len(train_image_file_list))]\n",
    "valid_image_file_list = list(np.array(train_image_file_list)[valid_image_file_index])\n",
    "train_image_file_list = list(np.array(train_image_file_list)[train_image_file_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFPlFa9FjtRn"
   },
   "outputs": [],
   "source": [
    "train_data_generator = custom_generator(train_image_file_list, train_data, \n",
    "                                         lb_label, continuous_var_list, categorical_var_list, \n",
    "                                         minmaxscaler, lb_list, \n",
    "                                         batch_size, mode = 'train', augment = datagen)\n",
    "\n",
    "validation_data_generator = custom_generator(valid_image_file_list, valid_data, \n",
    "                                             lb_label, continuous_var_list, categorical_var_list, \n",
    "                                             minmaxscaler, lb_list, \n",
    "                                             batch_size, mode = 'train', augment = None)\n",
    "\n",
    "test_data_generator = custom_generator(test_image_file_list, test_data_csv, \n",
    "                                         lb_label, continuous_var_list, categorical_var_list, \n",
    "                                         minmaxscaler, lb_list, \n",
    "                                         batch_size, mode = 'eval', augment = datagen_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0dexiFhFuxzI"
   },
   "outputs": [],
   "source": [
    "num_categorical_var = 0\n",
    "for i in range(len(categorical_var_list)):\n",
    "  num_categorical_var += len(train_data[categorical_var_list[i]].unique())\n",
    "total_num_var = len(continuous_var_list) + num_categorical_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ft8FE-w5jevW"
   },
   "source": [
    "## Learning rate finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQtNg1n2kFGO"
   },
   "source": [
    "I am using a code posted at [`pyimagesearch.com`](https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAN-VjdNkEme"
   },
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "    def __init__(self, model, stopFactor=4, beta=0.98):\n",
    "        # store the model, stop factor, and beta value (for computing\n",
    "        # a smoothed, average loss)\n",
    "        self.model = model\n",
    "        self.stopFactor = stopFactor\n",
    "        self.beta = beta\n",
    " \n",
    "        # initialize our list of learning rates and losses,\n",
    "        # respectively\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "\n",
    "        # initialize our learning rate multiplier, average loss, best\n",
    "        # loss found thus far, current batch number, and weights file\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "\n",
    "    def reset(self):\n",
    "        # re-initialize all variables from our constructor\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "    \n",
    "    def is_data_iter(self, data):\n",
    "        # define the set of class types we will check for\n",
    "        iterClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\", \"DataFrameIterator\", \"Iterator\", \"Sequence\"]\n",
    "\n",
    "        # return whether our data is an iterator\n",
    "        return data.__class__.__name__ in iterClasses\n",
    "  \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # grab the current learning rate and add log it to the list of\n",
    "        # learning rates that we've tried\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # grab the loss at the end of this batch, increment the total\n",
    "        # number of batches processed, compute the average average\n",
    "        # loss, smooth it, and update the losses list with the\n",
    "        # smoothed value\n",
    "        l = logs[\"loss\"]\n",
    "        self.batchNum += 1\n",
    "        self.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
    "        smooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
    "        self.losses.append(smooth)\n",
    "\n",
    "        # compute the maximum loss stopping factor value\n",
    "        stopLoss = self.stopFactor * self.bestLoss\n",
    "\n",
    "        # check to see whether the loss has grown too large\n",
    "        if self.batchNum > 1 and smooth > stopLoss:\n",
    "            # stop returning and return from the method\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        # check to see if the best loss should be updated\n",
    "        if self.batchNum == 1 or smooth < self.bestLoss:\n",
    "            self.bestLoss = smooth\n",
    "\n",
    "        # increase the learning rate\n",
    "        lr *= self.lrMult\n",
    "        K.set_value(self.model.optimizer.lr, lr)  \n",
    "\n",
    "    def find(self, trainData, startLR, endLR, epochs=None,\n",
    "        stepsPerEpoch=None, batchSize=32, sampleSize=2048,\n",
    "        verbose=1, useGen = True):\n",
    "        # reset our class-specific variables\n",
    "        self.reset()\n",
    "\n",
    "        # # determine if we are using a data generator or not\n",
    "        # useGen = self.is_data_iter(trainData)\n",
    "\n",
    "        # if we're using a generator and the steps per epoch is not\n",
    "        # supplied, raise an error\n",
    "        if useGen and stepsPerEpoch is None:\n",
    "            msg = \"Using generator without supplying stepsPerEpoch\"\n",
    "            raise Exception(msg)\n",
    "\n",
    "        # if we're not using a generator then our entire dataset must\n",
    "        # already be in memory\n",
    "        elif not useGen:\n",
    "            # grab the number of samples in the training data and\n",
    "            # then derive the number of steps per epoch\n",
    "            numSamples = len(trainData[0])\n",
    "            stepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
    "\n",
    "        # if no number of training epochs are supplied, compute the\n",
    "        # training epochs based on a default sample size\n",
    "        if epochs is None:\n",
    "            epochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
    "\n",
    "        # compute the total number of batch updates that will take\n",
    "        # place while we are attempting to find a good starting\n",
    "        # learning rate\n",
    "        numBatchUpdates = epochs * stepsPerEpoch\n",
    "\n",
    "        # derive the learning rate multiplier based on the ending\n",
    "        # learning rate, starting learning rate, and total number of\n",
    "        # batch updates\n",
    "        self.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
    "\n",
    "        # create a temporary file path for the model weights and\n",
    "        # then save the weights (so we can reset the weights when we\n",
    "        # are done)\n",
    "        self.weightsFile = tempfile.mkstemp()[1]\n",
    "        self.model.save_weights(self.weightsFile)\n",
    "\n",
    "        # grab the *original* learning rate (so we can reset it\n",
    "        # later), and then set the *starting* learning rate\n",
    "        origLR = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, startLR)\n",
    "\n",
    "        # construct a callback that will be called at the end of each\n",
    "        # batch, enabling us to increase our learning rate as training\n",
    "        # progresses\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs:\n",
    "            self.on_batch_end(batch, logs))\n",
    "\n",
    "        # check to see if we are using a data iterator\n",
    "        if useGen:\n",
    "            self.model.fit_generator(\n",
    "                trainData,\n",
    "                steps_per_epoch=stepsPerEpoch,\n",
    "                epochs=epochs,\n",
    "                verbose=verbose,\n",
    "                callbacks=[callback])\n",
    "\n",
    "        # otherwise, our entire training data is already in memory\n",
    "        else:\n",
    "            # train our model using Keras' fit method\n",
    "            self.model.fit(\n",
    "                trainData[0], trainData[1],\n",
    "                batch_size=batchSize,\n",
    "                epochs=epochs,\n",
    "                callbacks=[callback],\n",
    "                verbose=verbose)\n",
    "\n",
    "        # restore the original model weights and learning rate\n",
    "        self.model.load_weights(self.weightsFile)\n",
    "        K.set_value(self.model.optimizer.lr, origLR)\n",
    "\n",
    "    def plot_loss(self, skipBegin=10, skipEnd=1, title=\"\"):\n",
    "        # grab the learning rate and losses values to plot\n",
    "        lrs = self.lrs[skipBegin:-skipEnd]\n",
    "        losses = self.losses[skipBegin:-skipEnd]\n",
    "\n",
    "        # plot the learning rate vs. loss\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning Rate (Log Scale)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "\n",
    "        # if the title is not empty, add it to the plot\n",
    "        if title != \"\":\n",
    "            plt.title(title)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7241,
     "status": "ok",
     "timestamp": 1574903230862,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "vjGRTezCZs_C",
    "outputId": "bf4919a5-99b4-4c60-c902-0051f3f489a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a MLP for numerical variables\n",
    "model_mlp = Sequential()\n",
    "for i in range(5):\n",
    "  model_mlp.add(Dense(512, input_dim = total_num_var, activation = 'relu'))\n",
    "  model_mlp.add(BatchNormalization())\n",
    "  model_mlp.add(Dropout(0.5))\n",
    "\n",
    "# Define a CNN for image data\n",
    "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = inputs\n",
    "\n",
    "filters = (64, 128, 256, 512)\n",
    "for (i, f) in enumerate(filters):\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "model_cnn = Model(inputs, x)\n",
    "\n",
    "combined_input = concatenate([model_cnn.output, model_mlp.output])\n",
    "\n",
    "x = Dense(256, activation = 'relu')(combined_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, activation = 'relu')(combined_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, activation = 'relu')(combined_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(5, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs = [model_cnn.input, model_mlp.input], outputs = x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 531,
     "status": "error",
     "timestamp": 1575781832680,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "NHAWvd66uc2d",
    "outputId": "abff9908-fb94-4040-888d-937d827c2f1b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-2f16da56e7ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model.compile(optimizer = opt,\n\u001b[0m\u001b[1;32m      3\u001b[0m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               metrics = ['accuracy'])\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr = 1e-10)\n",
    "model.compile(optimizer = opt,\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2267068,
     "status": "ok",
     "timestamp": 1574905503593,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "EdT3E6d6m5ZC",
    "outputId": "02189111-653d-4b59-b888-83bc51776367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Epoch 1/5\n",
      "348/348 [==============================] - 527s 2s/step - loss: 2.6492 - acc: 0.2028\n",
      "Epoch 2/5\n",
      "348/348 [==============================] - 507s 1s/step - loss: 2.6658 - acc: 0.2022\n",
      "Epoch 3/5\n",
      "348/348 [==============================] - 508s 1s/step - loss: 2.4638 - acc: 0.2331\n",
      "Epoch 4/5\n",
      "348/348 [==============================] - 507s 1s/step - loss: 1.2688 - acc: 0.5608\n",
      "Epoch 5/5\n",
      "144/348 [===========>..................] - ETA: 4:54 - loss: 2.5112 - acc: 0.5393"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKRGIiKS5/w9PW/gH2AkZlAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrf = LearningRateFinder(model)\n",
    "lrf.find(\n",
    "  train_data_generator,\n",
    "  1e-10, 1e+1,\n",
    "  stepsPerEpoch = total_train // batch_size,\n",
    "  batchSize = batch_size,\n",
    "  epochs = 5)\n",
    "\n",
    "# plot the loss for the various learning rates and save the\n",
    "# resulting plot to disk\n",
    "lrf.plot_loss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AC1DJALR2HsB"
   },
   "source": [
    "### Retrain model with CLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1429,
     "status": "ok",
     "timestamp": 1574907333504,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "ovGpC7fh2fYd",
    "outputId": "80b8deec-23b7-4677-8c67-4a63db6704a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iWMt-pyC0f9"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVKMJ1pAd3hi"
   },
   "source": [
    "### Retrain model with CLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZkxG0dSd3hk"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r_VM7Vxrq_F7"
   },
   "outputs": [],
   "source": [
    "# Define a MLP for numerical variables\n",
    "model_mlp = Sequential()\n",
    "for i in range(5):\n",
    "  model_mlp.add(Dense(512, input_dim = total_num_var, activation = 'relu'))\n",
    "  model_mlp.add(BatchNormalization())\n",
    "  model_mlp.add(Dropout(0.5))\n",
    "\n",
    "# Define a CNN for image data\n",
    "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = inputs\n",
    "\n",
    "filters = (64, 128, 256, 512)\n",
    "for (i, f) in enumerate(filters):\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "model_cnn = Model(inputs, x)\n",
    "\n",
    "combined_input = concatenate([model_cnn.output, model_mlp.output])\n",
    "\n",
    "x = Dense(256, activation = 'relu')(combined_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, activation = 'relu')(combined_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, activation = 'relu')(combined_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(5, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs = [model_cnn.input, model_mlp.input], outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUZfQVJ7d3hn"
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr = 1e-5)\n",
    "\n",
    "model.compile(optimizer = opt,\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qi0oO7Nsd3hq"
   },
   "outputs": [],
   "source": [
    "clr = CyclicLR(\n",
    "\tmode = 'triangular2',\n",
    "\tbase_lr = 1e-5,\n",
    "\tmax_lr = 1e-2,\n",
    "\tstep_size = 4 * (total_train // batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pS1YztR4d3ht"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(path, 'model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCmx6LBNd3hu"
   },
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath = checkpoint_path, \n",
    "    verbose = 1, \n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    save_best_only = True)\n",
    "\n",
    "# define an early stopping rule\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss',\n",
    "                                        mode = 'min',\n",
    "                                        verbose = 1,\n",
    "                                        patience = 20)\n",
    "\n",
    "# define class weights to account for imbalance in training data\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_label),\n",
    "                                                 train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7939728,
     "status": "ok",
     "timestamp": 1574646147629,
     "user": {
      "displayName": "YUKIKO SUZUKI",
      "photoUrl": "",
      "userId": "07554899533181101057"
     },
     "user_tz": 360
    },
    "id": "K9C1fuuLd3hw",
    "outputId": "b782653b-0cdb-4e05-e70b-ab70ae887fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "348/348 [==============================] - 313s 900ms/step - loss: 1.6489 - acc: 0.4622 - val_loss: 2.0917 - val_acc: 0.6053\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.09168, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 2/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.8773 - acc: 0.6680 - val_loss: 1.7574 - val_acc: 0.5471\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.09168 to 1.75739, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 3/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.8212 - acc: 0.6800 - val_loss: 1.2204 - val_acc: 0.6048\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.75739 to 1.22041, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 4/150\n",
      "348/348 [==============================] - 305s 877ms/step - loss: 0.7897 - acc: 0.7020 - val_loss: 0.7597 - val_acc: 0.6959\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.22041 to 0.75965, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 5/150\n",
      "348/348 [==============================] - 304s 874ms/step - loss: 0.7518 - acc: 0.7096 - val_loss: 0.7081 - val_acc: 0.7274\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.75965 to 0.70814, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 6/150\n",
      "348/348 [==============================] - 304s 874ms/step - loss: 0.7180 - acc: 0.7249 - val_loss: 0.7465 - val_acc: 0.7080\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.70814\n",
      "Epoch 7/150\n",
      "348/348 [==============================] - 304s 874ms/step - loss: 0.6870 - acc: 0.7346 - val_loss: 0.6198 - val_acc: 0.7616\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.70814 to 0.61976, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 8/150\n",
      "348/348 [==============================] - 305s 877ms/step - loss: 0.6592 - acc: 0.7432 - val_loss: 0.5812 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.61976 to 0.58124, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 9/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.6419 - acc: 0.7515 - val_loss: 0.5895 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.58124\n",
      "Epoch 10/150\n",
      "348/348 [==============================] - 305s 878ms/step - loss: 0.6463 - acc: 0.7466 - val_loss: 0.6694 - val_acc: 0.7395\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.58124\n",
      "Epoch 11/150\n",
      "348/348 [==============================] - 305s 877ms/step - loss: 0.6755 - acc: 0.7401 - val_loss: 0.7761 - val_acc: 0.6977\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.58124\n",
      "Epoch 12/150\n",
      "348/348 [==============================] - 306s 878ms/step - loss: 0.7070 - acc: 0.7259 - val_loss: 0.9109 - val_acc: 0.6592\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.58124\n",
      "Epoch 13/150\n",
      "348/348 [==============================] - 305s 876ms/step - loss: 0.6904 - acc: 0.7381 - val_loss: 0.8553 - val_acc: 0.6501\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.58124\n",
      "Epoch 14/150\n",
      "348/348 [==============================] - 305s 877ms/step - loss: 0.6688 - acc: 0.7417 - val_loss: 0.6602 - val_acc: 0.7357\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.58124\n",
      "Epoch 15/150\n",
      "348/348 [==============================] - 307s 881ms/step - loss: 0.6480 - acc: 0.7519 - val_loss: 0.6261 - val_acc: 0.7546\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.58124\n",
      "Epoch 16/150\n",
      "348/348 [==============================] - 305s 877ms/step - loss: 0.6191 - acc: 0.7592 - val_loss: 0.5669 - val_acc: 0.7810\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.58124 to 0.56689, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 17/150\n",
      "348/348 [==============================] - 306s 878ms/step - loss: 0.6116 - acc: 0.7638 - val_loss: 0.6025 - val_acc: 0.7678\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.56689\n",
      "Epoch 18/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.6221 - acc: 0.7619 - val_loss: 0.5920 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.56689\n",
      "Epoch 19/150\n",
      "348/348 [==============================] - 305s 877ms/step - loss: 0.6393 - acc: 0.7520 - val_loss: 0.7110 - val_acc: 0.7228\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.56689\n",
      "Epoch 20/150\n",
      "348/348 [==============================] - 305s 877ms/step - loss: 0.6581 - acc: 0.7438 - val_loss: 1.1854 - val_acc: 0.5294\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.56689\n",
      "Epoch 21/150\n",
      "348/348 [==============================] - 305s 876ms/step - loss: 0.6694 - acc: 0.7387 - val_loss: 0.6516 - val_acc: 0.7392\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.56689\n",
      "Epoch 22/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.6467 - acc: 0.7490 - val_loss: 0.6097 - val_acc: 0.7619\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.56689\n",
      "Epoch 23/150\n",
      "348/348 [==============================] - 305s 878ms/step - loss: 0.6252 - acc: 0.7578 - val_loss: 0.5997 - val_acc: 0.7627\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.56689\n",
      "Epoch 24/150\n",
      "348/348 [==============================] - 306s 878ms/step - loss: 0.5974 - acc: 0.7704 - val_loss: 0.5453 - val_acc: 0.7907\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.56689 to 0.54532, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 25/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.5985 - acc: 0.7681 - val_loss: 0.5582 - val_acc: 0.7791\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.54532\n",
      "Epoch 26/150\n",
      "348/348 [==============================] - 306s 878ms/step - loss: 0.5974 - acc: 0.7694 - val_loss: 0.5716 - val_acc: 0.7777\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.54532\n",
      "Epoch 27/150\n",
      "348/348 [==============================] - 306s 878ms/step - loss: 0.6149 - acc: 0.7650 - val_loss: 0.6402 - val_acc: 0.7435\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.54532\n",
      "Epoch 28/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.6177 - acc: 0.7592 - val_loss: 0.5917 - val_acc: 0.7686\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.54532\n",
      "Epoch 29/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.6343 - acc: 0.7540 - val_loss: 0.5754 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.54532\n",
      "Epoch 30/150\n",
      "348/348 [==============================] - 306s 878ms/step - loss: 0.6064 - acc: 0.7678 - val_loss: 0.5753 - val_acc: 0.7732\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.54532\n",
      "Epoch 31/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.5927 - acc: 0.7688 - val_loss: 0.5400 - val_acc: 0.7912\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.54532 to 0.54000, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 32/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.5698 - acc: 0.7843 - val_loss: 0.5193 - val_acc: 0.7996\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.54000 to 0.51928, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 33/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5696 - acc: 0.7746 - val_loss: 0.5357 - val_acc: 0.7961\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51928\n",
      "Epoch 34/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.5703 - acc: 0.7815 - val_loss: 0.5303 - val_acc: 0.7942\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51928\n",
      "Epoch 35/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5756 - acc: 0.7759 - val_loss: 0.5502 - val_acc: 0.7791\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51928\n",
      "Epoch 36/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5811 - acc: 0.7704 - val_loss: 0.5506 - val_acc: 0.7883\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51928\n",
      "Epoch 37/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5840 - acc: 0.7732 - val_loss: 0.5902 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.51928\n",
      "Epoch 38/150\n",
      "348/348 [==============================] - 307s 881ms/step - loss: 0.5716 - acc: 0.7777 - val_loss: 0.5251 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.51928\n",
      "Epoch 39/150\n",
      "348/348 [==============================] - 307s 881ms/step - loss: 0.5549 - acc: 0.7843 - val_loss: 0.5202 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.51928\n",
      "Epoch 40/150\n",
      "348/348 [==============================] - 307s 881ms/step - loss: 0.5395 - acc: 0.7887 - val_loss: 0.5057 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.51928 to 0.50568, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 41/150\n",
      "348/348 [==============================] - 307s 882ms/step - loss: 0.5335 - acc: 0.7935 - val_loss: 0.5123 - val_acc: 0.8060\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.50568\n",
      "Epoch 42/150\n",
      "348/348 [==============================] - 307s 881ms/step - loss: 0.5306 - acc: 0.7985 - val_loss: 0.5098 - val_acc: 0.8074\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.50568\n",
      "Epoch 43/150\n",
      "348/348 [==============================] - 307s 881ms/step - loss: 0.5406 - acc: 0.7918 - val_loss: 0.5196 - val_acc: 0.7936\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.50568\n",
      "Epoch 44/150\n",
      "348/348 [==============================] - 307s 881ms/step - loss: 0.5480 - acc: 0.7871 - val_loss: 0.5240 - val_acc: 0.7988\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.50568\n",
      "Epoch 45/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.5444 - acc: 0.7873 - val_loss: 0.5321 - val_acc: 0.7918\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.50568\n",
      "Epoch 46/150\n",
      "348/348 [==============================] - 306s 878ms/step - loss: 0.5351 - acc: 0.7936 - val_loss: 0.5058 - val_acc: 0.8052\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.50568\n",
      "Epoch 47/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.5254 - acc: 0.7972 - val_loss: 0.5071 - val_acc: 0.8101\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.50568\n",
      "Epoch 48/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.5206 - acc: 0.7960 - val_loss: 0.4955 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.50568 to 0.49546, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 49/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5167 - acc: 0.7979 - val_loss: 0.4881 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.49546 to 0.48811, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 50/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5193 - acc: 0.8016 - val_loss: 0.4975 - val_acc: 0.8068\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.48811\n",
      "Epoch 51/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5205 - acc: 0.7966 - val_loss: 0.5075 - val_acc: 0.8103\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.48811\n",
      "Epoch 52/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5151 - acc: 0.8027 - val_loss: 0.5052 - val_acc: 0.8112\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.48811\n",
      "Epoch 53/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.5188 - acc: 0.8005 - val_loss: 0.5057 - val_acc: 0.8004\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.48811\n",
      "Epoch 54/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5061 - acc: 0.8048 - val_loss: 0.5020 - val_acc: 0.8077\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.48811\n",
      "Epoch 55/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5043 - acc: 0.8050 - val_loss: 0.4831 - val_acc: 0.8198\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.48811 to 0.48309, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 56/150\n",
      "348/348 [==============================] - 307s 881ms/step - loss: 0.4933 - acc: 0.8087 - val_loss: 0.4894 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.48309\n",
      "Epoch 57/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4998 - acc: 0.8097 - val_loss: 0.4890 - val_acc: 0.8190\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.48309\n",
      "Epoch 58/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5017 - acc: 0.8029 - val_loss: 0.4880 - val_acc: 0.8160\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.48309\n",
      "Epoch 59/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5003 - acc: 0.8024 - val_loss: 0.4883 - val_acc: 0.8203\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.48309\n",
      "Epoch 60/150\n",
      "348/348 [==============================] - 308s 884ms/step - loss: 0.5016 - acc: 0.8096 - val_loss: 0.4868 - val_acc: 0.8147\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.48309\n",
      "Epoch 61/150\n",
      "348/348 [==============================] - 306s 881ms/step - loss: 0.5008 - acc: 0.8056 - val_loss: 0.4877 - val_acc: 0.8200\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.48309\n",
      "Epoch 62/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.5055 - acc: 0.8055 - val_loss: 0.4974 - val_acc: 0.8128\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.48309\n",
      "Epoch 63/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4978 - acc: 0.8058 - val_loss: 0.4876 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.48309\n",
      "Epoch 64/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4893 - acc: 0.8115 - val_loss: 0.4798 - val_acc: 0.8219\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.48309 to 0.47975, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 65/150\n",
      "348/348 [==============================] - 307s 881ms/step - loss: 0.4909 - acc: 0.8096 - val_loss: 0.4859 - val_acc: 0.8187\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.47975\n",
      "Epoch 66/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4941 - acc: 0.8088 - val_loss: 0.4833 - val_acc: 0.8211\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.47975\n",
      "Epoch 67/150\n",
      "348/348 [==============================] - 306s 881ms/step - loss: 0.4950 - acc: 0.8054 - val_loss: 0.4722 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.47975 to 0.47221, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 68/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4947 - acc: 0.8093 - val_loss: 0.4857 - val_acc: 0.8173\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.47221\n",
      "Epoch 69/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4917 - acc: 0.8093 - val_loss: 0.4881 - val_acc: 0.8179\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.47221\n",
      "Epoch 70/150\n",
      "348/348 [==============================] - 307s 882ms/step - loss: 0.4914 - acc: 0.8130 - val_loss: 0.4760 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.47221\n",
      "Epoch 71/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4874 - acc: 0.8137 - val_loss: 0.4861 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.47221\n",
      "Epoch 72/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4929 - acc: 0.8090 - val_loss: 0.4682 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.47221 to 0.46821, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_cp_20191210.h5\n",
      "Epoch 73/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4807 - acc: 0.8152 - val_loss: 0.4911 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.46821\n",
      "Epoch 74/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4815 - acc: 0.8148 - val_loss: 0.4777 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.46821\n",
      "Epoch 75/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4850 - acc: 0.8136 - val_loss: 0.4715 - val_acc: 0.8227\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.46821\n",
      "Epoch 76/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4872 - acc: 0.8104 - val_loss: 0.4887 - val_acc: 0.8179\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.46821\n",
      "Epoch 77/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4856 - acc: 0.8100 - val_loss: 0.4759 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.46821\n",
      "Epoch 78/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4865 - acc: 0.8145 - val_loss: 0.4930 - val_acc: 0.8147\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.46821\n",
      "Epoch 79/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4915 - acc: 0.8112 - val_loss: 0.4704 - val_acc: 0.8241\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.46821\n",
      "Epoch 80/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4808 - acc: 0.8158 - val_loss: 0.4772 - val_acc: 0.8252\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.46821\n",
      "Epoch 81/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4804 - acc: 0.8118 - val_loss: 0.4818 - val_acc: 0.8249\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.46821\n",
      "Epoch 82/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4824 - acc: 0.8141 - val_loss: 0.4784 - val_acc: 0.8184\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.46821\n",
      "Epoch 83/150\n",
      "348/348 [==============================] - 306s 880ms/step - loss: 0.4845 - acc: 0.8121 - val_loss: 0.4845 - val_acc: 0.8233\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.46821\n",
      "Epoch 84/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4891 - acc: 0.8148 - val_loss: 0.4818 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.46821\n",
      "Epoch 85/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4775 - acc: 0.8129 - val_loss: 0.4803 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.46821\n",
      "Epoch 86/150\n",
      "348/348 [==============================] - 306s 879ms/step - loss: 0.4775 - acc: 0.8155 - val_loss: 0.4781 - val_acc: 0.8222\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.46821\n",
      "Epoch 87/150\n",
      "338/348 [============================>.] - ETA: 7s - loss: 0.4883 - acc: 0.8138"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_data_generator,\n",
    "    steps_per_epoch = total_train // batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_data_generator,\n",
    "    validation_steps = total_valid // batch_size,\n",
    "    callbacks = [clr, early_stopping_callback, cp_callback],\n",
    "    class_weight = class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPYTy6hy2Fgf"
   },
   "source": [
    "## Prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 595,
     "status": "error",
     "timestamp": 1576100005413,
     "user": {
      "displayName": "MIZUHIRO SUZUKI",
      "photoUrl": "",
      "userId": "11994014812088599130"
     },
     "user_tz": 360
    },
    "id": "EsB5cizlpct9",
    "outputId": "c5c68182-94d7-421b-ef99-4d4360aaa84e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-af35551b060f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m datagen_test = ImageDataGenerator(rescale = 1./255,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                   \u001b[0mshear_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0mzoom_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0mhorizontal_flip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mvertical_flip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "datagen_test = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  rotation_range=10.,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                  height_shift_range = 0.1,\n",
    "                                  featurewise_center = True)\n",
    "datagen_test.mean = [train_data.R_mean.mean() / 255, train_data.G_mean.mean() / 255, train_data.B_mean.mean() / 255]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4221406,
     "status": "ok",
     "timestamp": 1574609872660,
     "user": {
      "displayName": "YUKIKO SUZUKI",
      "photoUrl": "",
      "userId": "07554899533181101057"
     },
     "user_tz": 360
    },
    "id": "dPhY2Kgwf5Qk",
    "outputId": "44667422-2b09-4a4b-8dbf-782d3a7016c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 113s 493ms/step\n",
      "229/229 [==============================] - 109s 476ms/step\n",
      "229/229 [==============================] - 109s 476ms/step\n",
      "229/229 [==============================] - 109s 475ms/step\n",
      "229/229 [==============================] - 109s 474ms/step\n",
      "229/229 [==============================] - 108s 472ms/step\n",
      "229/229 [==============================] - 106s 462ms/step\n",
      "229/229 [==============================] - 106s 465ms/step\n",
      "229/229 [==============================] - 106s 464ms/step\n",
      "229/229 [==============================] - 107s 465ms/step\n",
      "229/229 [==============================] - 106s 462ms/step\n",
      "229/229 [==============================] - 106s 463ms/step\n",
      "229/229 [==============================] - 106s 461ms/step\n",
      "229/229 [==============================] - 106s 461ms/step\n",
      "229/229 [==============================] - 107s 468ms/step\n",
      "229/229 [==============================] - 107s 467ms/step\n",
      "229/229 [==============================] - 108s 470ms/step\n",
      "229/229 [==============================] - 106s 462ms/step\n",
      "229/229 [==============================] - 106s 464ms/step\n",
      "229/229 [==============================] - 107s 468ms/step\n"
     ]
    }
   ],
   "source": [
    "tta_steps = 20\n",
    "predictions = []\n",
    "\n",
    "for i in range(tta_steps):\n",
    "    print(i)\n",
    "    test_data_generator = custom_generator(test_image_file_list, test_data_csv, \n",
    "                                          lb_label, continuous_var_list, categorical_var_list, \n",
    "                                          minmaxscaler, lb_list, \n",
    "                                          batch_size, mode = 'eval', augment = datagen_test)\n",
    "    preds = model.predict_generator(test_data_generator, \n",
    "                                    steps = total_test // batch_size + 1,\n",
    "                                    verbose = 1)\n",
    "    predictions.append(preds)\n",
    "\n",
    "pred = np.mean(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiJm8xxurQAH"
   },
   "outputs": [],
   "source": [
    "pred_pd = pd.DataFrame({'id': test_data_csv.id})\n",
    "\n",
    "test_image_file_list = glob(os.path.join(path, 'image_resize/test/*.jpg'))\n",
    "pred_id_temp = [os.path.basename(test_image_file_list[i]).replace('.jpg', '') for i in range(total_test)]\n",
    "pred_pd_temp = pd.DataFrame({'id': pred_id_temp})\n",
    "pred_pd_temp['concrete_cement'] = pred[:, 0]\n",
    "pred_pd_temp['healthy_metal'] = pred[:, 1]\n",
    "pred_pd_temp['incomplete'] = pred[:, 2]\n",
    "pred_pd_temp['irregular_metal'] = pred[:, 3]\n",
    "pred_pd_temp['other'] = pred[:, 4]\n",
    "\n",
    "pred_pd = pd.merge(pred_pd, pred_pd_temp, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H23UGWdswFOQ"
   },
   "outputs": [],
   "source": [
    "pred_pd.to_csv(os.path.join(path, 'submission/mixed_inputs_resize.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPYTy6hy2Fgf"
   },
   "source": [
    "## Prediction for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 595,
     "status": "error",
     "timestamp": 1576100005413,
     "user": {
      "displayName": "MIZUHIRO SUZUKI",
      "photoUrl": "",
      "userId": "11994014812088599130"
     },
     "user_tz": 360
    },
    "id": "EsB5cizlpct9",
    "outputId": "c5c68182-94d7-421b-ef99-4d4360aaa84e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-af35551b060f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m datagen_test = ImageDataGenerator(rescale = 1./255,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                   \u001b[0mshear_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0mzoom_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0mhorizontal_flip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mvertical_flip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "datagen_train = ImageDataGenerator(rescale = 1./255,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  rotation_range=10.,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                  height_shift_range = 0.1,\n",
    "                                  featurewise_center = True)\n",
    "datagen_train.mean = [train_data.R_mean.mean() / 255, train_data.G_mean.mean() / 255, train_data.B_mean.mean() / 255]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4221406,
     "status": "ok",
     "timestamp": 1574609872660,
     "user": {
      "displayName": "YUKIKO SUZUKI",
      "photoUrl": "",
      "userId": "07554899533181101057"
     },
     "user_tz": 360
    },
    "id": "dPhY2Kgwf5Qk",
    "outputId": "44667422-2b09-4a4b-8dbf-782d3a7016c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229/229 [==============================] - 113s 493ms/step\n",
      "229/229 [==============================] - 109s 476ms/step\n",
      "229/229 [==============================] - 109s 476ms/step\n",
      "229/229 [==============================] - 109s 475ms/step\n",
      "229/229 [==============================] - 109s 474ms/step\n",
      "229/229 [==============================] - 108s 472ms/step\n",
      "229/229 [==============================] - 106s 462ms/step\n",
      "229/229 [==============================] - 106s 465ms/step\n",
      "229/229 [==============================] - 106s 464ms/step\n",
      "229/229 [==============================] - 107s 465ms/step\n",
      "229/229 [==============================] - 106s 462ms/step\n",
      "229/229 [==============================] - 106s 463ms/step\n",
      "229/229 [==============================] - 106s 461ms/step\n",
      "229/229 [==============================] - 106s 461ms/step\n",
      "229/229 [==============================] - 107s 468ms/step\n",
      "229/229 [==============================] - 107s 467ms/step\n",
      "229/229 [==============================] - 108s 470ms/step\n",
      "229/229 [==============================] - 106s 462ms/step\n",
      "229/229 [==============================] - 106s 464ms/step\n",
      "229/229 [==============================] - 107s 468ms/step\n"
     ]
    }
   ],
   "source": [
    "tta_steps = 20\n",
    "predictions = []\n",
    "train_image_file_list = glob(os.path.join(path, 'image_resize/train/*.jpg'))\n",
    "total_train = len(train_image_file_list)\n",
    "\n",
    "for i in range(tta_steps):\n",
    "    print(i)\n",
    "    test_data_generator = custom_generator(train_image_file_list, train_data_csv, \n",
    "                                          lb_label, continuous_var_list, categorical_var_list, \n",
    "                                          minmaxscaler, lb_list, \n",
    "                                          batch_size, mode = 'eval', augment = datagen_train)\n",
    "    preds = model.predict_generator(train_data_generator, \n",
    "                                    steps = total_train // batch_size + 1,\n",
    "                                    verbose = 1)\n",
    "    predictions.append(preds)\n",
    "\n",
    "pred = np.mean(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiJm8xxurQAH"
   },
   "outputs": [],
   "source": [
    "pred_pd = pd.DataFrame({'id': test_data_csv.id})\n",
    "\n",
    "train_image_file_list = glob(os.path.join(path, 'image_resize/train/*.jpg'))\n",
    "pred_id_temp = [os.path.basename(test_image_file_list[i]).replace('.jpg', '') for i in range(total_test)]\n",
    "pred_pd_temp = pd.DataFrame({'id': pred_id_temp})\n",
    "pred_pd_temp['concrete_cement'] = pred[:, 0]\n",
    "pred_pd_temp['healthy_metal'] = pred[:, 1]\n",
    "pred_pd_temp['incomplete'] = pred[:, 2]\n",
    "pred_pd_temp['irregular_metal'] = pred[:, 3]\n",
    "pred_pd_temp['other'] = pred[:, 4]\n",
    "\n",
    "pred_pd = pd.merge(pred_pd, pred_pd_temp, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H23UGWdswFOQ"
   },
   "outputs": [],
   "source": [
    "pred_pd.to_csv(os.path.join(path, 'feature/mixed_inputs_resize.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mixed_inputs_cnn_resize.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
