{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZIjavb0Vb21"
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2947,
     "status": "ok",
     "timestamp": 1576087681475,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "nwbCYEDnVeVW",
    "outputId": "f6e83916-0e91-490d-ffd0-e68e692279f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from keras import datasets, layers, models\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LambdaCallback, Callback\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image as krs_image\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Flatten, Input, concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "import random\n",
    "from glob import glob\n",
    "import tempfile\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLBnFge9fFaZ"
   },
   "source": [
    "## Generator to load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBjJHoRBL7LP"
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_generator(images_list, dataframe, lb_label, continuous_var_list, categorical_var_list, minmaxscaler, lb_list, batch_size, mode, augment = None):\n",
    "    i = 0\n",
    "    # if not evaluation generator, shuffle the image \n",
    "    if mode == 'train':\n",
    "        random.shuffle(images_list)\n",
    "    while True:        \n",
    "        images = []\n",
    "        csv_continuous_features = []\n",
    "        csv_categorical_features = []\n",
    "        labels = []\n",
    "        \n",
    "        while len(images) < batch_size:\n",
    "            if i == len(images_list):\n",
    "                # if evaluation generator, break the loop when the last image is retrieved\n",
    "                if mode == 'eval':\n",
    "                    break\n",
    "                i = 0\n",
    "                random.shuffle(images_list)                \n",
    "                  \n",
    "            # Read image from list and convert to array\n",
    "            image_path = images_list[i]\n",
    "            image_name = os.path.basename(image_path).replace('.jpg', '')\n",
    "            image = krs_image.load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "            image = np.asarray(image)\n",
    "            images.append(image)\n",
    "\n",
    "            # Read data from csv using the name of current image\n",
    "            csv_row = dataframe[dataframe.id == image_name]\n",
    "            \n",
    "            # extract continuous features\n",
    "            csv_continuous_features.append(np.array(csv_row[continuous_var_list])[0])\n",
    "            \n",
    "            # extract categorical features\n",
    "            csv_categorical_features.append(np.array(csv_row[categorical_var_list])[0])\n",
    "\n",
    "            # # just to check if data are correctly retrieved...\n",
    "            # print(image_name)\n",
    "            \n",
    "            if mode == 'train':\n",
    "                label = np.array(csv_row['label'])[0]\n",
    "                labels.append(label)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        images = np.array(images)\n",
    "        if augment != None:\n",
    "            if mode == 'train':\n",
    "                (images, labels) = next(augment.flow(images, labels, batch_size = batch_size, shuffle = False))\n",
    "            elif mode == 'eval':\n",
    "                images = next(augment.flow(images, batch_size = batch_size, shuffle = False))\n",
    "        elif augment == None:\n",
    "            datagen_rescale = ImageDataGenerator(rescale = 1 / 255., featurewise_center = True)\n",
    "            datagen_rescale.mean = [train_data.H_mean.mean() / 255, train_data.S_mean.mean() / 255, train_data.V_mean.mean() / 255]\n",
    "            if mode == 'train':\n",
    "                (images, labels) = next(datagen_rescale.flow(images, labels, batch_size = batch_size, shuffle = False))\n",
    "            elif mode == 'eval':\n",
    "                images = next(datagen_rescale.flow(images, batch_size = batch_size, shuffle = False))\n",
    "            \n",
    "        # rescale continuous features\n",
    "        csv_continuous_features = minmaxscaler.transform(np.array(csv_continuous_features))\n",
    "        \n",
    "        # convert categorical features into one-hot encoding\n",
    "        csv_categorical_features_temp = lb_list[0].transform(np.array(csv_categorical_features)[:, 0])\n",
    "        \n",
    "        if len(categorical_var_list) > 0:\n",
    "            for j in range(1, len(categorical_var_list)):\n",
    "                np.hstack(csv_categorical_features_temp, lb_list[j].transform(csv_categorical_features_temp[:, j]))\n",
    "        csv_categorical_features = csv_categorical_features_temp\n",
    "        \n",
    "        csv = np.hstack([csv_continuous_features, csv_categorical_features])\n",
    "        \n",
    "        if mode == 'train':\n",
    "            labels = lb_label.transform(labels)\n",
    "            yield [np.array(images), csv], labels\n",
    "        elif mode == 'eval':\n",
    "            yield [np.array(images), csv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hktipl5GL7ku"
   },
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hqUBGgC9h2Yr"
   },
   "source": [
    "### Model for training with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 831,
     "status": "error",
     "timestamp": 1576093575124,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "Cy_F8orGfHey",
    "outputId": "0060f315-55e2-4826-cdbd-8001067065ac"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-17c020687778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mIMG_WIDTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/stac/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image_mask_hsv/train/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'image_mask_hsv/test/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 48\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "path = '/content/drive/My Drive/stac/'\n",
    "train_dir = os.path.join(path, 'image_mask_hsv/train/')\n",
    "test_dir = os.path.join(path, 'image_mask_hsv/test/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6VhUyLQftMU"
   },
   "outputs": [],
   "source": [
    "train_data_csv = pd.read_csv(os.path.join(path, 'image_mask_hsv/train/train_data_with_dist.csv'))\n",
    "test_data_csv = pd.read_csv(os.path.join(path, 'image_mask_hsv/test/test_data_with_dist.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36488,
     "status": "ok",
     "timestamp": 1576087934531,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "KvAxsdsmYTXE",
    "outputId": "a04a16c9-0325-4ace-e18c-0c9c156da370"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>centroid_x</th>\n",
       "      <th>centroid_y</th>\n",
       "      <th>log_building_area</th>\n",
       "      <th>building_vertices</th>\n",
       "      <th>building_width</th>\n",
       "      <th>building_height</th>\n",
       "      <th>place</th>\n",
       "      <th>R_mean</th>\n",
       "      <th>G_mean</th>\n",
       "      <th>B_mean</th>\n",
       "      <th>R_std</th>\n",
       "      <th>G_std</th>\n",
       "      <th>B_std</th>\n",
       "      <th>H_mean</th>\n",
       "      <th>S_mean</th>\n",
       "      <th>V_mean</th>\n",
       "      <th>H_std</th>\n",
       "      <th>S_std</th>\n",
       "      <th>V_std</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>distance_10_all</th>\n",
       "      <th>distance_20_all</th>\n",
       "      <th>distance_50_all</th>\n",
       "      <th>distance_10_train</th>\n",
       "      <th>distance_20_train</th>\n",
       "      <th>distance_50_train</th>\n",
       "      <th>concrete_10_train</th>\n",
       "      <th>healthy_10_train</th>\n",
       "      <th>incomplete_10_train</th>\n",
       "      <th>irregular_10_train</th>\n",
       "      <th>other_10_train</th>\n",
       "      <th>concrete_20_train</th>\n",
       "      <th>healthy_20_train</th>\n",
       "      <th>incomplete_20_train</th>\n",
       "      <th>irregular_20_train</th>\n",
       "      <th>other_20_train</th>\n",
       "      <th>concrete_50_train</th>\n",
       "      <th>healthy_50_train</th>\n",
       "      <th>incomplete_50_train</th>\n",
       "      <th>irregular_50_train</th>\n",
       "      <th>other_50_train</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7a3f2a10</td>\n",
       "      <td>concrete_cement</td>\n",
       "      <td>-74.158686</td>\n",
       "      <td>4.555175</td>\n",
       "      <td>-18.936547</td>\n",
       "      <td>5</td>\n",
       "      <td>9.549665</td>\n",
       "      <td>13.229584</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>77.854895</td>\n",
       "      <td>73.351518</td>\n",
       "      <td>146.993500</td>\n",
       "      <td>67.880480</td>\n",
       "      <td>64.434540</td>\n",
       "      <td>125.473623</td>\n",
       "      <td>0.063230</td>\n",
       "      <td>0.054169</td>\n",
       "      <td>0.316736</td>\n",
       "      <td>0.056303</td>\n",
       "      <td>0.072556</td>\n",
       "      <td>0.274796</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>7a3f2a10.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7a1f731e</td>\n",
       "      <td>irregular_metal</td>\n",
       "      <td>-74.158750</td>\n",
       "      <td>4.555195</td>\n",
       "      <td>-18.939354</td>\n",
       "      <td>5</td>\n",
       "      <td>10.043447</td>\n",
       "      <td>12.529615</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>124.694521</td>\n",
       "      <td>124.628628</td>\n",
       "      <td>147.602022</td>\n",
       "      <td>108.855020</td>\n",
       "      <td>109.155474</td>\n",
       "      <td>125.356539</td>\n",
       "      <td>0.271230</td>\n",
       "      <td>0.022647</td>\n",
       "      <td>0.493386</td>\n",
       "      <td>0.253855</td>\n",
       "      <td>0.036350</td>\n",
       "      <td>0.429687</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>7a1f731e.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7a424ad8</td>\n",
       "      <td>healthy_metal</td>\n",
       "      <td>-74.158802</td>\n",
       "      <td>4.555230</td>\n",
       "      <td>-19.628413</td>\n",
       "      <td>5</td>\n",
       "      <td>7.561388</td>\n",
       "      <td>7.970262</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>135.498841</td>\n",
       "      <td>136.810501</td>\n",
       "      <td>154.154495</td>\n",
       "      <td>109.534520</td>\n",
       "      <td>110.653357</td>\n",
       "      <td>123.881052</td>\n",
       "      <td>0.323498</td>\n",
       "      <td>0.020789</td>\n",
       "      <td>0.537306</td>\n",
       "      <td>0.267187</td>\n",
       "      <td>0.018401</td>\n",
       "      <td>0.434195</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>7a424ad8.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7a3edc5e</td>\n",
       "      <td>healthy_metal</td>\n",
       "      <td>-74.158993</td>\n",
       "      <td>4.554755</td>\n",
       "      <td>-18.914724</td>\n",
       "      <td>5</td>\n",
       "      <td>10.414648</td>\n",
       "      <td>12.632232</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>135.939801</td>\n",
       "      <td>135.257038</td>\n",
       "      <td>144.207639</td>\n",
       "      <td>119.120671</td>\n",
       "      <td>118.611700</td>\n",
       "      <td>125.867072</td>\n",
       "      <td>0.263625</td>\n",
       "      <td>0.021276</td>\n",
       "      <td>0.533524</td>\n",
       "      <td>0.236370</td>\n",
       "      <td>0.019511</td>\n",
       "      <td>0.467427</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.568182</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>7a3edc5e.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7a303a6e</td>\n",
       "      <td>healthy_metal</td>\n",
       "      <td>-74.158795</td>\n",
       "      <td>4.554937</td>\n",
       "      <td>-19.027574</td>\n",
       "      <td>5</td>\n",
       "      <td>13.587928</td>\n",
       "      <td>10.044892</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>104.500974</td>\n",
       "      <td>102.593758</td>\n",
       "      <td>124.069303</td>\n",
       "      <td>108.094900</td>\n",
       "      <td>106.310129</td>\n",
       "      <td>126.937176</td>\n",
       "      <td>0.182812</td>\n",
       "      <td>0.017346</td>\n",
       "      <td>0.410758</td>\n",
       "      <td>0.200787</td>\n",
       "      <td>0.021575</td>\n",
       "      <td>0.424660</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>7a303a6e.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        id  ... other_50_train      filename\n",
       "0           0  7a3f2a10  ...       0.076923  7a3f2a10.jpg\n",
       "1           1  7a1f731e  ...       0.071429  7a1f731e.jpg\n",
       "2           2  7a424ad8  ...       0.066667  7a424ad8.jpg\n",
       "3           3  7a3edc5e  ...       0.022727  7a3edc5e.jpg\n",
       "4           4  7a303a6e  ...       0.032258  7a303a6e.jpg\n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_csv['filename'] = train_data_csv.id + '.jpg'\n",
    "train_data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36252,
     "status": "ok",
     "timestamp": 1576087934532,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "zqt4dr-6anK5",
    "outputId": "51e617d4-aa35-4c8f-f30f-4de40cf4855b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>centroid_x</th>\n",
       "      <th>centroid_y</th>\n",
       "      <th>log_building_area</th>\n",
       "      <th>building_vertices</th>\n",
       "      <th>building_width</th>\n",
       "      <th>building_height</th>\n",
       "      <th>place</th>\n",
       "      <th>R_mean</th>\n",
       "      <th>G_mean</th>\n",
       "      <th>B_mean</th>\n",
       "      <th>R_std</th>\n",
       "      <th>G_std</th>\n",
       "      <th>B_std</th>\n",
       "      <th>H_mean</th>\n",
       "      <th>S_mean</th>\n",
       "      <th>V_mean</th>\n",
       "      <th>H_std</th>\n",
       "      <th>S_std</th>\n",
       "      <th>V_std</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>distance_10_all</th>\n",
       "      <th>distance_20_all</th>\n",
       "      <th>distance_50_all</th>\n",
       "      <th>distance_10_train</th>\n",
       "      <th>distance_20_train</th>\n",
       "      <th>distance_50_train</th>\n",
       "      <th>concrete_10_train</th>\n",
       "      <th>healthy_10_train</th>\n",
       "      <th>incomplete_10_train</th>\n",
       "      <th>irregular_10_train</th>\n",
       "      <th>other_10_train</th>\n",
       "      <th>concrete_20_train</th>\n",
       "      <th>healthy_20_train</th>\n",
       "      <th>incomplete_20_train</th>\n",
       "      <th>irregular_20_train</th>\n",
       "      <th>other_20_train</th>\n",
       "      <th>concrete_50_train</th>\n",
       "      <th>healthy_50_train</th>\n",
       "      <th>incomplete_50_train</th>\n",
       "      <th>irregular_50_train</th>\n",
       "      <th>other_50_train</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7a4d630a</td>\n",
       "      <td>-74.158953</td>\n",
       "      <td>4.554654</td>\n",
       "      <td>-19.139941</td>\n",
       "      <td>5</td>\n",
       "      <td>10.039401</td>\n",
       "      <td>9.375537</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>123.915329</td>\n",
       "      <td>127.008717</td>\n",
       "      <td>160.728248</td>\n",
       "      <td>94.568756</td>\n",
       "      <td>96.901490</td>\n",
       "      <td>122.426289</td>\n",
       "      <td>0.333620</td>\n",
       "      <td>0.055182</td>\n",
       "      <td>0.499930</td>\n",
       "      <td>0.260862</td>\n",
       "      <td>0.050494</td>\n",
       "      <td>0.381395</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>36</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>7a4d630a.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7a4bbbd6</td>\n",
       "      <td>-74.159509</td>\n",
       "      <td>4.554677</td>\n",
       "      <td>-19.125909</td>\n",
       "      <td>5</td>\n",
       "      <td>8.930837</td>\n",
       "      <td>13.331169</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>103.174164</td>\n",
       "      <td>98.903974</td>\n",
       "      <td>129.441246</td>\n",
       "      <td>102.110107</td>\n",
       "      <td>98.049362</td>\n",
       "      <td>126.917853</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.023418</td>\n",
       "      <td>0.405354</td>\n",
       "      <td>0.144369</td>\n",
       "      <td>0.027564</td>\n",
       "      <td>0.400830</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>7a4bbbd6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7a4ac744</td>\n",
       "      <td>-74.158910</td>\n",
       "      <td>4.555028</td>\n",
       "      <td>-19.116754</td>\n",
       "      <td>5</td>\n",
       "      <td>8.372523</td>\n",
       "      <td>12.637685</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>122.421879</td>\n",
       "      <td>119.100460</td>\n",
       "      <td>146.136707</td>\n",
       "      <td>107.017083</td>\n",
       "      <td>104.601871</td>\n",
       "      <td>125.512256</td>\n",
       "      <td>0.189898</td>\n",
       "      <td>0.020047</td>\n",
       "      <td>0.480388</td>\n",
       "      <td>0.173220</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.419911</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>7a4ac744.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7a4881fa</td>\n",
       "      <td>-74.158861</td>\n",
       "      <td>4.555007</td>\n",
       "      <td>-19.178307</td>\n",
       "      <td>5</td>\n",
       "      <td>7.844693</td>\n",
       "      <td>12.018867</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>112.187198</td>\n",
       "      <td>107.693765</td>\n",
       "      <td>153.989872</td>\n",
       "      <td>95.124870</td>\n",
       "      <td>92.243979</td>\n",
       "      <td>124.043331</td>\n",
       "      <td>0.113918</td>\n",
       "      <td>0.040184</td>\n",
       "      <td>0.448283</td>\n",
       "      <td>0.135922</td>\n",
       "      <td>0.046959</td>\n",
       "      <td>0.376538</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>7a4881fa.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7a4aa4a8</td>\n",
       "      <td>-74.158777</td>\n",
       "      <td>4.554996</td>\n",
       "      <td>-18.840272</td>\n",
       "      <td>5</td>\n",
       "      <td>14.114200</td>\n",
       "      <td>10.788166</td>\n",
       "      <td>borde_rural</td>\n",
       "      <td>109.659702</td>\n",
       "      <td>108.746657</td>\n",
       "      <td>133.855902</td>\n",
       "      <td>106.682311</td>\n",
       "      <td>106.374756</td>\n",
       "      <td>126.848866</td>\n",
       "      <td>0.207272</td>\n",
       "      <td>0.032812</td>\n",
       "      <td>0.436992</td>\n",
       "      <td>0.231152</td>\n",
       "      <td>0.049795</td>\n",
       "      <td>0.422201</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>7a4aa4a8.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        id  ...  other_50_train      filename\n",
       "0           0  7a4d630a  ...        0.024390  7a4d630a.jpg\n",
       "1           1  7a4bbbd6  ...        0.021739  7a4bbbd6.jpg\n",
       "2           2  7a4ac744  ...        0.032258  7a4ac744.jpg\n",
       "3           3  7a4881fa  ...        0.032258  7a4881fa.jpg\n",
       "4           4  7a4aa4a8  ...        0.037037  7a4aa4a8.jpg\n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_csv['filename'] = test_data_csv.id + '.jpg'\n",
    "test_data_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 353,
     "status": "error",
     "timestamp": 1576018028156,
     "user": {
      "displayName": "MIZUHIRO SUZUKI",
      "photoUrl": "",
      "userId": "11994014812088599130"
     },
     "user_tz": 360
    },
    "id": "XK9rBNX3al5g",
    "outputId": "730128be-9931-48cd-f54e-c0e6405f8748"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c4118b4de02e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_data, valid_data, train_label, valid_label = train_test_split(train_data_csv, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                                     \u001b[0mtrain_data_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                                     \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                     stratify = train_data_csv.label)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, train_label, valid_label = train_test_split(train_data_csv, \n",
    "                                                                    train_data_csv.label, \n",
    "                                                                    test_size = 0.25, \n",
    "                                                                    stratify = train_data_csv.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nd67ejcVn4oi"
   },
   "outputs": [],
   "source": [
    "total_train = len(train_data)\n",
    "total_valid = len(valid_data)\n",
    "total_test = len(test_data_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tr2S6f00iKEY"
   },
   "source": [
    "#### Preparation for loading data with generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oz2CEyM1iIq8"
   },
   "outputs": [],
   "source": [
    "# Specify which variables are used in NN\n",
    "\n",
    "# continuous_var_list = ['centroid_x', 'centroid_y', 'log_building_area', 'building_vertices', 'building_width', 'building_height']\n",
    "continuous_var_list = ['log_building_area', 'building_vertices', 'building_width', 'building_height',\n",
    "                        'R_mean', 'G_mean', 'B_mean', 'R_std', 'G_std', 'B_std',\n",
    "                        'H_mean', 'S_mean', 'V_mean', 'H_std', 'S_std', 'V_std',\n",
    "                        'distance_10_all', 'distance_20_all', 'distance_50_all',\n",
    "                        'concrete_10_train', 'healthy_10_train', 'incomplete_10_train', 'irregular_10_train', 'other_10_train',\n",
    "                        'concrete_20_train', 'healthy_20_train', 'incomplete_20_train', 'irregular_20_train', 'other_20_train',\n",
    "                       'concrete_50_train', 'healthy_50_train', 'incomplete_50_train', 'irregular_50_train', 'other_50_train']\n",
    "categorical_var_list = ['place']\n",
    "\n",
    "# Define a rescaler so that continuous variables are within the range of [0, 1]\n",
    "minmaxscaler = MinMaxScaler()\n",
    "minmaxscaler.fit(train_data_csv[continuous_var_list])\n",
    "\n",
    "# Define one-hot encoders for categorical variables\n",
    "lb0 = LabelBinarizer()\n",
    "lb0.fit(train_data_csv[categorical_var_list[0]])\n",
    "lb_list = [lb0]\n",
    "\n",
    "# Define a one-hot encoder for label\n",
    "lb_label = LabelBinarizer()\n",
    "lb_label.fit(train_data_csv.label)\n",
    "\n",
    "# Create an empty data generator, which will be used in training data generator\n",
    "datagen = ImageDataGenerator(rescale = 1 / 255.,\n",
    "                            horizontal_flip = True,\n",
    "                            vertical_flip = True,\n",
    "                            rotation_range = 45,\n",
    "                            featurewise_center = True)\n",
    "datagen.mean = [train_data.H_mean.mean() / 255., train_data.S_mean.mean() / 255., train_data.V_mean.mean() / 255.]\n",
    "\n",
    "datagen_test = ImageDataGenerator(rescale = 1 / 255.,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  rotation_range=10.,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                  height_shift_range = 0.1,\n",
    "                                  featurewise_center = True)\n",
    "datagen_test.mean = [train_data.H_mean.mean() / 255., train_data.S_mean.mean() / 255., train_data.V_mean.mean() / 255.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJv4SilgjUzK"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the image list and csv\n",
    "path = '/content/drive/My Drive/stac/'\n",
    "train_image_file_list = glob(os.path.join(path, 'image_mask_hsv/train/*.jpg'))\n",
    "test_image_file_list = glob(os.path.join(path, 'image_mask_hsv/test/*.jpg'))\n",
    "\n",
    "valid_image_file_index = [os.path.basename(train_image_file_list[i]).replace('.jpg','') in list(valid_data.id) for i in range(len(train_image_file_list))]\n",
    "train_image_file_index = [os.path.basename(train_image_file_list[i]).replace('.jpg','') in list(train_data.id) for i in range(len(train_image_file_list))]\n",
    "valid_image_file_list = list(np.array(train_image_file_list)[valid_image_file_index])\n",
    "train_image_file_list = list(np.array(train_image_file_list)[train_image_file_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFPlFa9FjtRn"
   },
   "outputs": [],
   "source": [
    "train_data_generator = custom_generator(train_image_file_list, train_data, \n",
    "                                         lb_label, continuous_var_list, categorical_var_list, \n",
    "                                         minmaxscaler, lb_list, \n",
    "                                         batch_size, mode = 'train', augment = datagen)\n",
    "\n",
    "validation_data_generator = custom_generator(valid_image_file_list, valid_data, \n",
    "                                             lb_label, continuous_var_list, categorical_var_list, \n",
    "                                             minmaxscaler, lb_list, \n",
    "                                             batch_size, mode = 'train', augment = None)\n",
    "\n",
    "test_data_generator = custom_generator(test_image_file_list, test_data_csv, \n",
    "                                         lb_label, continuous_var_list, categorical_var_list, \n",
    "                                         minmaxscaler, lb_list, \n",
    "                                         batch_size, mode = 'eval', augment = datagen_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0dexiFhFuxzI"
   },
   "outputs": [],
   "source": [
    "num_categorical_var = 0\n",
    "for i in range(len(categorical_var_list)):\n",
    "  num_categorical_var += len(train_data[categorical_var_list[i]].unique())\n",
    "total_num_var = len(continuous_var_list) + num_categorical_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ft8FE-w5jevW"
   },
   "source": [
    "## Learning rate finder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQtNg1n2kFGO"
   },
   "source": [
    "I am using a code posted at [`pyimagesearch.com`](https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAN-VjdNkEme"
   },
   "outputs": [],
   "source": [
    "class LearningRateFinder:\n",
    "    def __init__(self, model, stopFactor=4, beta=0.98):\n",
    "        # store the model, stop factor, and beta value (for computing\n",
    "        # a smoothed, average loss)\n",
    "        self.model = model\n",
    "        self.stopFactor = stopFactor\n",
    "        self.beta = beta\n",
    " \n",
    "        # initialize our list of learning rates and losses,\n",
    "        # respectively\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "\n",
    "        # initialize our learning rate multiplier, average loss, best\n",
    "        # loss found thus far, current batch number, and weights file\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "\n",
    "    def reset(self):\n",
    "        # re-initialize all variables from our constructor\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.lrMult = 1\n",
    "        self.avgLoss = 0\n",
    "        self.bestLoss = 1e9\n",
    "        self.batchNum = 0\n",
    "        self.weightsFile = None\n",
    "    \n",
    "    def is_data_iter(self, data):\n",
    "        # define the set of class types we will check for\n",
    "        iterClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\", \"DataFrameIterator\", \"Iterator\", \"Sequence\"]\n",
    "\n",
    "        # return whether our data is an iterator\n",
    "        return data.__class__.__name__ in iterClasses\n",
    "  \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        # grab the current learning rate and add log it to the list of\n",
    "        # learning rates that we've tried\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        # grab the loss at the end of this batch, increment the total\n",
    "        # number of batches processed, compute the average average\n",
    "        # loss, smooth it, and update the losses list with the\n",
    "        # smoothed value\n",
    "        l = logs[\"loss\"]\n",
    "        self.batchNum += 1\n",
    "        self.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
    "        smooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
    "        self.losses.append(smooth)\n",
    "\n",
    "        # compute the maximum loss stopping factor value\n",
    "        stopLoss = self.stopFactor * self.bestLoss\n",
    "\n",
    "        # check to see whether the loss has grown too large\n",
    "        if self.batchNum > 1 and smooth > stopLoss:\n",
    "            # stop returning and return from the method\n",
    "            self.model.stop_training = True\n",
    "            return\n",
    "\n",
    "        # check to see if the best loss should be updated\n",
    "        if self.batchNum == 1 or smooth < self.bestLoss:\n",
    "            self.bestLoss = smooth\n",
    "\n",
    "        # increase the learning rate\n",
    "        lr *= self.lrMult\n",
    "        K.set_value(self.model.optimizer.lr, lr)  \n",
    "\n",
    "    def find(self, trainData, startLR, endLR, epochs=None,\n",
    "        stepsPerEpoch=None, batchSize=32, sampleSize=2048,\n",
    "        verbose=1, useGen = True):\n",
    "        # reset our class-specific variables\n",
    "        self.reset()\n",
    "\n",
    "        # # determine if we are using a data generator or not\n",
    "        # useGen = self.is_data_iter(trainData)\n",
    "\n",
    "        # if we're using a generator and the steps per epoch is not\n",
    "        # supplied, raise an error\n",
    "        if useGen and stepsPerEpoch is None:\n",
    "            msg = \"Using generator without supplying stepsPerEpoch\"\n",
    "            raise Exception(msg)\n",
    "\n",
    "        # if we're not using a generator then our entire dataset must\n",
    "        # already be in memory\n",
    "        elif not useGen:\n",
    "            # grab the number of samples in the training data and\n",
    "            # then derive the number of steps per epoch\n",
    "            numSamples = len(trainData[0])\n",
    "            stepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
    "\n",
    "        # if no number of training epochs are supplied, compute the\n",
    "        # training epochs based on a default sample size\n",
    "        if epochs is None:\n",
    "            epochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
    "\n",
    "        # compute the total number of batch updates that will take\n",
    "        # place while we are attempting to find a good starting\n",
    "        # learning rate\n",
    "        numBatchUpdates = epochs * stepsPerEpoch\n",
    "\n",
    "        # derive the learning rate multiplier based on the ending\n",
    "        # learning rate, starting learning rate, and total number of\n",
    "        # batch updates\n",
    "        self.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
    "\n",
    "        # create a temporary file path for the model weights and\n",
    "        # then save the weights (so we can reset the weights when we\n",
    "        # are done)\n",
    "        self.weightsFile = tempfile.mkstemp()[1]\n",
    "        self.model.save_weights(self.weightsFile)\n",
    "\n",
    "        # grab the *original* learning rate (so we can reset it\n",
    "        # later), and then set the *starting* learning rate\n",
    "        origLR = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, startLR)\n",
    "\n",
    "        # construct a callback that will be called at the end of each\n",
    "        # batch, enabling us to increase our learning rate as training\n",
    "        # progresses\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs:\n",
    "            self.on_batch_end(batch, logs))\n",
    "\n",
    "        # check to see if we are using a data iterator\n",
    "        if useGen:\n",
    "            self.model.fit_generator(\n",
    "                trainData,\n",
    "                steps_per_epoch=stepsPerEpoch,\n",
    "                epochs=epochs,\n",
    "                verbose=verbose,\n",
    "                callbacks=[callback])\n",
    "\n",
    "        # otherwise, our entire training data is already in memory\n",
    "        else:\n",
    "            # train our model using Keras' fit method\n",
    "            self.model.fit(\n",
    "                trainData[0], trainData[1],\n",
    "                batch_size=batchSize,\n",
    "                epochs=epochs,\n",
    "                callbacks=[callback],\n",
    "                verbose=verbose)\n",
    "\n",
    "        # restore the original model weights and learning rate\n",
    "        self.model.load_weights(self.weightsFile)\n",
    "        K.set_value(self.model.optimizer.lr, origLR)\n",
    "\n",
    "    def plot_loss(self, skipBegin=10, skipEnd=1, title=\"\"):\n",
    "        # grab the learning rate and losses values to plot\n",
    "        lrs = self.lrs[skipBegin:-skipEnd]\n",
    "        losses = self.losses[skipBegin:-skipEnd]\n",
    "\n",
    "        # plot the learning rate vs. loss\n",
    "        plt.plot(lrs, losses)\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning Rate (Log Scale)\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "\n",
    "        # if the title is not empty, add it to the plot\n",
    "        if title != \"\":\n",
    "            plt.title(title)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hm10WFcgjE6F"
   },
   "outputs": [],
   "source": [
    "# Define a MLP for numerical variables\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(512, input_dim = total_num_var, activation = 'relu'))\n",
    "model_mlp.add(BatchNormalization())\n",
    "model_mlp.add(Dropout(0.5))\n",
    "model_mlp.add(Dense(256, activation = 'relu'))\n",
    "model_mlp.add(BatchNormalization())\n",
    "model_mlp.add(Dropout(0.5))\n",
    "model_mlp.add(Dense(128, activation = 'relu'))\n",
    "model_mlp.add(BatchNormalization())\n",
    "model_mlp.add(Dropout(0.5))\n",
    "model_mlp.add(Dense(64, activation = 'relu'))\n",
    "model_mlp.add(BatchNormalization())\n",
    "model_mlp.add(Dropout(0.5))\n",
    "\n",
    "# Define a CNN for image data\n",
    "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = inputs\n",
    "\n",
    "filters = (64, 128, 256, 512)\n",
    "for (i, f) in enumerate(filters):\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "model_cnn = Model(inputs, x)\n",
    "\n",
    "combined_input = concatenate([model_cnn.output, model_mlp.output])\n",
    "\n",
    "x = Dense(256, activation = 'relu')(combined_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(5, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs = [model_cnn.input, model_mlp.input], outputs = x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1422,
     "status": "ok",
     "timestamp": 1574568254943,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "NHAWvd66uc2d",
    "outputId": "997b3e2b-9e0e-4d49-da16-20f40c19f841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 224, 224, 64) 1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 224, 224, 64) 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 224, 224, 64) 36928       batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 224, 224, 64) 256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 224, 224, 64) 36928       batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 224, 224, 64) 256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 112, 112, 64) 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 112, 112, 64) 0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 112, 112, 128 73856       dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 112, 112, 128 512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 112, 112, 128 147584      batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 112, 112, 128 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 112, 112, 128 147584      batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 112, 112, 128 512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 56, 56, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 56, 56, 128)  0           max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 56, 56, 256)  295168      dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 56, 56, 256)  1024        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 56, 56, 256)  590080      batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 56, 56, 256)  1024        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 56, 56, 256)  590080      batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 56, 56, 256)  1024        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 28, 28, 256)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 28, 28, 256)  0           max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 28, 28, 512)  1180160     dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 28, 28, 512)  2048        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 28, 28, 512)  2359808     batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_21_input (InputLayer)     (None, 39)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 28, 28, 512)  2048        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 512)          20480       dense_21_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 28, 28, 512)  2359808     batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 512)          2048        dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 28, 28, 512)  2048        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 512)          0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 14, 14, 512)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 512)          262656      dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 14, 14, 512)  0           max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 512)          2048        dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 100352)       0           dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 512)          0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 256)          25690368    flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 512)          262656      dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 256)          1024        dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 512)          2048        dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 256)          0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 512)          0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 256)          65792       dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 512)          262656      dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 256)          1024        dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 512)          2048        dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 256)          0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 512)          0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 256)          65792       dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 512)          262656      dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 256)          1024        dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 512)          2048        dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 256)          0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 512)          0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 768)          0           dropout_35[0][0]                 \n",
      "                                                                 dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 256)          196864      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 256)          1024        dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 256)          0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 256)          65792       dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 256)          1024        dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 256)          0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 256)          65792       dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 256)          1024        dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 256)          0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 5)            1285        dropout_38[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 35,070,469\n",
      "Trainable params: 35,056,517\n",
      "Non-trainable params: 13,952\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr = 1e-10)\n",
    "model.compile(optimizer = opt,\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 52712,
     "status": "error",
     "timestamp": 1574568306854,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "EdT3E6d6m5ZC",
    "outputId": "61917154-21f9-4842-81bb-753f1964cb8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-1fe4a76d0252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mstepsPerEpoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_train\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   epochs = 5)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# plot the loss for the various learning rates and save the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-d366d26c4335>\u001b[0m in \u001b[0;36mfind\u001b[0;34m(self, trainData, startLR, endLR, epochs, stepsPerEpoch, batchSize, sampleSize, verbose, useGen)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 callbacks=[callback])\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# otherwise, our entire training data is already in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, op, message)\u001b[0m\n\u001b[1;32m    363\u001b[0m   \"\"\"\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;34m\"\"\"Creates a `ResourceExhaustedError`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     super(ResourceExhaustedError, self).__init__(node_def, op, message,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lrf = LearningRateFinder(model)\n",
    "lrf.find(\n",
    "  train_data_generator,\n",
    "  1e-10, 1e+1,\n",
    "  stepsPerEpoch = total_train // batch_size,\n",
    "  batchSize = batch_size,\n",
    "  epochs = 5)\n",
    "\n",
    "# plot the loss for the various learning rates and save the\n",
    "# resulting plot to disk\n",
    "lrf.plot_loss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AC1DJALR2HsB"
   },
   "source": [
    "### Retrain model with CLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovGpC7fh2fYd"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iWMt-pyC0f9"
   },
   "outputs": [],
   "source": [
    "class CyclicLR(Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hVKMJ1pAd3hi"
   },
   "source": [
    "### Retrain model with CLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZkxG0dSd3hk"
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRIRQ5YweMZA"
   },
   "outputs": [],
   "source": [
    "# Define a MLP for numerical variables\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(512, input_dim = total_num_var, activation = 'relu'))\n",
    "model_mlp.add(BatchNormalization())\n",
    "model_mlp.add(Dropout(0.5))\n",
    "model_mlp.add(Dense(256, activation = 'relu'))\n",
    "model_mlp.add(BatchNormalization())\n",
    "model_mlp.add(Dropout(0.5))\n",
    "model_mlp.add(Dense(128, activation = 'relu'))\n",
    "model_mlp.add(BatchNormalization())\n",
    "model_mlp.add(Dropout(0.5))\n",
    "model_mlp.add(Dense(64, activation = 'relu'))\n",
    "model_mlp.add(BatchNormalization())\n",
    "model_mlp.add(Dropout(0.5))\n",
    "\n",
    "# Define a CNN for image data\n",
    "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x = inputs\n",
    "\n",
    "filters = (64, 128, 256, 512)\n",
    "for (i, f) in enumerate(filters):\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = Conv2D(f, (3, 3), activation = 'relu', padding = 'same')(x)\n",
    "  x = BatchNormalization(axis = -1)(x)\n",
    "  x = MaxPooling2D(pool_size = (2, 2))(x)\n",
    "  x = Dropout(0.2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "model_cnn = Model(inputs, x)\n",
    "\n",
    "combined_input = concatenate([model_cnn.output, model_mlp.output])\n",
    "\n",
    "x = Dense(256, activation = 'relu')(combined_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(5, activation = 'softmax')(x)\n",
    "\n",
    "model = Model(inputs = [model_cnn.input, model_mlp.input], outputs = x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUZfQVJ7d3hn"
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr = 1e-4)\n",
    "\n",
    "model.compile(optimizer = opt,\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qi0oO7Nsd3hq"
   },
   "outputs": [],
   "source": [
    "clr = CyclicLR(\n",
    "\tmode = 'triangular2',\n",
    "\tbase_lr = 1e-4,\n",
    "\tmax_lr = 1e-2,\n",
    "\tstep_size = 4 * (total_train // batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pS1YztR4d3ht"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(path, 'model_cp/mixed_inputs_cnn/mixed_inputs_mask_hsv_cp.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCmx6LBNd3hu"
   },
   "outputs": [],
   "source": [
    "# Create a callback that saves the model's weights\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath = checkpoint_path, \n",
    "    verbose = 1, \n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min',\n",
    "    save_best_only = True)\n",
    "\n",
    "# define an early stopping rule\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss',\n",
    "                                        mode = 'min',\n",
    "                                        verbose = 1,\n",
    "                                        patience = 10)\n",
    "\n",
    "# define class weights to account for imbalance in training data\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_label),\n",
    "                                                 train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 12690104,
     "status": "ok",
     "timestamp": 1574642808872,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "K9C1fuuLd3hw",
    "outputId": "cc124a23-8802-42f7-9bb9-afdc1613b9f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "232/232 [==============================] - 286s 1s/step - loss: 0.6186 - acc: 0.7670 - val_loss: 0.6730 - val_acc: 0.7302\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.67300, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 2/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.6772 - acc: 0.7412 - val_loss: 0.8623 - val_acc: 0.6959\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.67300\n",
      "Epoch 3/100\n",
      "232/232 [==============================] - 272s 1s/step - loss: 0.6916 - acc: 0.7364 - val_loss: 0.8627 - val_acc: 0.6981\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.67300\n",
      "Epoch 4/100\n",
      "232/232 [==============================] - 271s 1s/step - loss: 0.6910 - acc: 0.7333 - val_loss: 0.8178 - val_acc: 0.6943\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.67300\n",
      "Epoch 5/100\n",
      "232/232 [==============================] - 271s 1s/step - loss: 0.6828 - acc: 0.7368 - val_loss: 0.6258 - val_acc: 0.7519\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.67300 to 0.62577, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 6/100\n",
      "232/232 [==============================] - 270s 1s/step - loss: 0.6553 - acc: 0.7477 - val_loss: 0.5682 - val_acc: 0.7792\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.62577 to 0.56819, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 7/100\n",
      "232/232 [==============================] - 267s 1s/step - loss: 0.6152 - acc: 0.7635 - val_loss: 0.5584 - val_acc: 0.7846\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.56819 to 0.55836, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 8/100\n",
      "232/232 [==============================] - 267s 1s/step - loss: 0.5996 - acc: 0.7692 - val_loss: 0.5378 - val_acc: 0.7987\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.55836 to 0.53776, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 9/100\n",
      "232/232 [==============================] - 265s 1s/step - loss: 0.5807 - acc: 0.7777 - val_loss: 0.5395 - val_acc: 0.7936\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.53776\n",
      "Epoch 10/100\n",
      "232/232 [==============================] - 263s 1s/step - loss: 0.5861 - acc: 0.7754 - val_loss: 0.5450 - val_acc: 0.7955\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.53776\n",
      "Epoch 11/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.5845 - acc: 0.7745 - val_loss: 0.5430 - val_acc: 0.7914\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.53776\n",
      "Epoch 12/100\n",
      "232/232 [==============================] - 267s 1s/step - loss: 0.6003 - acc: 0.7707 - val_loss: 0.5801 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.53776\n",
      "Epoch 13/100\n",
      "232/232 [==============================] - 266s 1s/step - loss: 0.6145 - acc: 0.7627 - val_loss: 0.5725 - val_acc: 0.7757\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.53776\n",
      "Epoch 14/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.5820 - acc: 0.7805 - val_loss: 0.5518 - val_acc: 0.7895\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.53776\n",
      "Epoch 15/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.5672 - acc: 0.7835 - val_loss: 0.5305 - val_acc: 0.7960\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.53776 to 0.53049, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 16/100\n",
      "232/232 [==============================] - 270s 1s/step - loss: 0.5422 - acc: 0.7918 - val_loss: 0.5222 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.53049 to 0.52215, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 17/100\n",
      "232/232 [==============================] - 271s 1s/step - loss: 0.5451 - acc: 0.7917 - val_loss: 0.5213 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.52215 to 0.52128, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 18/100\n",
      "232/232 [==============================] - 272s 1s/step - loss: 0.5408 - acc: 0.7945 - val_loss: 0.5377 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.52128\n",
      "Epoch 19/100\n",
      "232/232 [==============================] - 271s 1s/step - loss: 0.5358 - acc: 0.7918 - val_loss: 0.5414 - val_acc: 0.7911\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.52128\n",
      "Epoch 20/100\n",
      "232/232 [==============================] - 271s 1s/step - loss: 0.5437 - acc: 0.7928 - val_loss: 0.5438 - val_acc: 0.7884\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.52128\n",
      "Epoch 21/100\n",
      "232/232 [==============================] - 269s 1s/step - loss: 0.5559 - acc: 0.7831 - val_loss: 0.5324 - val_acc: 0.8033\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.52128\n",
      "Epoch 22/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.5381 - acc: 0.7947 - val_loss: 0.5393 - val_acc: 0.7892\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.52128\n",
      "Epoch 23/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.5167 - acc: 0.8047 - val_loss: 0.5126 - val_acc: 0.8006\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.52128 to 0.51255, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 24/100\n",
      "232/232 [==============================] - 273s 1s/step - loss: 0.5085 - acc: 0.8077 - val_loss: 0.5226 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.51255\n",
      "Epoch 25/100\n",
      "232/232 [==============================] - 271s 1s/step - loss: 0.5079 - acc: 0.8042 - val_loss: 0.5241 - val_acc: 0.7963\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.51255\n",
      "Epoch 26/100\n",
      "232/232 [==============================] - 272s 1s/step - loss: 0.4931 - acc: 0.8112 - val_loss: 0.5185 - val_acc: 0.8052\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.51255\n",
      "Epoch 27/100\n",
      "232/232 [==============================] - 271s 1s/step - loss: 0.5035 - acc: 0.8044 - val_loss: 0.5186 - val_acc: 0.7998\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.51255\n",
      "Epoch 28/100\n",
      "232/232 [==============================] - 270s 1s/step - loss: 0.5083 - acc: 0.8049 - val_loss: 0.5118 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.51255 to 0.51175, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 29/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.5045 - acc: 0.8035 - val_loss: 0.5320 - val_acc: 0.7976\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.51175\n",
      "Epoch 30/100\n",
      "232/232 [==============================] - 266s 1s/step - loss: 0.4956 - acc: 0.8110 - val_loss: 0.5237 - val_acc: 0.8014\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.51175\n",
      "Epoch 31/100\n",
      "232/232 [==============================] - 266s 1s/step - loss: 0.4867 - acc: 0.8124 - val_loss: 0.5316 - val_acc: 0.8001\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.51175\n",
      "Epoch 32/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.4891 - acc: 0.8140 - val_loss: 0.5184 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.51175\n",
      "Epoch 33/100\n",
      "232/232 [==============================] - 269s 1s/step - loss: 0.4771 - acc: 0.8147 - val_loss: 0.5205 - val_acc: 0.8003\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.51175\n",
      "Epoch 34/100\n",
      "232/232 [==============================] - 269s 1s/step - loss: 0.4836 - acc: 0.8139 - val_loss: 0.5183 - val_acc: 0.7995\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.51175\n",
      "Epoch 35/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.4891 - acc: 0.8158 - val_loss: 0.5319 - val_acc: 0.7965\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.51175\n",
      "Epoch 36/100\n",
      "232/232 [==============================] - 269s 1s/step - loss: 0.4794 - acc: 0.8148 - val_loss: 0.5356 - val_acc: 0.7903\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.51175\n",
      "Epoch 37/100\n",
      "232/232 [==============================] - 267s 1s/step - loss: 0.4765 - acc: 0.8151 - val_loss: 0.4968 - val_acc: 0.8139\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.51175 to 0.49684, saving model to /content/drive/My Drive/stac/model_cp/mixed_inputs_cnn/mixed_inputs_resize_hsv_cp_20191123.h5\n",
      "Epoch 38/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.4780 - acc: 0.8170 - val_loss: 0.5395 - val_acc: 0.7968\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.49684\n",
      "Epoch 39/100\n",
      "232/232 [==============================] - 270s 1s/step - loss: 0.4852 - acc: 0.8152 - val_loss: 0.5000 - val_acc: 0.8028\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.49684\n",
      "Epoch 40/100\n",
      "232/232 [==============================] - 268s 1s/step - loss: 0.4719 - acc: 0.8198 - val_loss: 0.5274 - val_acc: 0.8009\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.49684\n",
      "Epoch 41/100\n",
      "232/232 [==============================] - 269s 1s/step - loss: 0.4796 - acc: 0.8198 - val_loss: 0.5186 - val_acc: 0.7984\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.49684\n",
      "Epoch 42/100\n",
      "232/232 [==============================] - 270s 1s/step - loss: 0.4661 - acc: 0.8190 - val_loss: 0.5192 - val_acc: 0.8049\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.49684\n",
      "Epoch 43/100\n",
      "232/232 [==============================] - 270s 1s/step - loss: 0.4727 - acc: 0.8173 - val_loss: 0.5338 - val_acc: 0.7968\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.49684\n",
      "Epoch 44/100\n",
      "232/232 [==============================] - 271s 1s/step - loss: 0.4711 - acc: 0.8174 - val_loss: 0.5173 - val_acc: 0.8022\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.49684\n",
      "Epoch 45/100\n",
      "232/232 [==============================] - 269s 1s/step - loss: 0.4767 - acc: 0.8189 - val_loss: 0.5060 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.49684\n",
      "Epoch 46/100\n",
      "232/232 [==============================] - 271s 1s/step - loss: 0.4652 - acc: 0.8239 - val_loss: 0.5181 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.49684\n",
      "Epoch 47/100\n",
      "232/232 [==============================] - 270s 1s/step - loss: 0.4619 - acc: 0.8216 - val_loss: 0.5267 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.49684\n",
      "Epoch 00047: early stopping\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_data_generator,\n",
    "    steps_per_epoch = total_train // batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_data_generator,\n",
    "    validation_steps = total_valid // batch_size,\n",
    "    callbacks = [clr, early_stopping_callback, cp_callback],\n",
    "    class_weight = class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPYTy6hy2Fgf"
   },
   "source": [
    "## Prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsB5cizlpct9"
   },
   "outputs": [],
   "source": [
    "datagen_test = ImageDataGenerator(rescale = 1 / 255.,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  rotation_range=10.,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                  height_shift_range = 0.1,\n",
    "                                  featurewise_center = True)\n",
    "datagen_test.mean = [train_data.H_mean.mean() / 255., train_data.S_mean.mean() / 255., train_data.V_mean.mean() / 255.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1215466,
     "status": "ok",
     "timestamp": 1574654653063,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "dPhY2Kgwf5Qk",
    "outputId": "19b6a71c-1bbd-45cf-be6c-71d804ffbfec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "153/153 [==============================] - 138s 901ms/step\n",
      "1\n",
      "153/153 [==============================] - 137s 894ms/step\n",
      "2\n",
      "153/153 [==============================] - 136s 889ms/step\n",
      "3\n",
      "153/153 [==============================] - 137s 895ms/step\n",
      "4\n",
      "153/153 [==============================] - 146s 954ms/step\n",
      "5\n",
      "153/153 [==============================] - 137s 893ms/step\n",
      "6\n",
      "153/153 [==============================] - 136s 891ms/step\n",
      "7\n",
      "153/153 [==============================] - 136s 892ms/step\n",
      "8\n",
      "153/153 [==============================] - 135s 882ms/step\n",
      "9\n",
      "153/153 [==============================] - 135s 883ms/step\n",
      "10\n",
      "153/153 [==============================] - 134s 879ms/step\n",
      "11\n",
      "153/153 [==============================] - 135s 882ms/step\n",
      "12\n",
      "153/153 [==============================] - 134s 876ms/step\n",
      "13\n",
      "153/153 [==============================] - 133s 868ms/step\n",
      "14\n",
      "153/153 [==============================] - 134s 875ms/step\n",
      "15\n",
      "153/153 [==============================] - 133s 867ms/step\n",
      "16\n",
      "153/153 [==============================] - 134s 879ms/step\n",
      "17\n",
      "153/153 [==============================] - 137s 895ms/step\n",
      "18\n",
      "153/153 [==============================] - 132s 862ms/step\n",
      "19\n",
      "153/153 [==============================] - 132s 863ms/step\n"
     ]
    }
   ],
   "source": [
    "tta_steps = 20\n",
    "predictions = []\n",
    "\n",
    "for i in range(tta_steps):\n",
    "    print(i)\n",
    "    test_data_generator = custom_generator(test_image_file_list, test_data_csv, \n",
    "                                          lb_label, continuous_var_list, categorical_var_list, \n",
    "                                          minmaxscaler, lb_list, \n",
    "                                          batch_size, mode = 'eval', augment = datagen_test)\n",
    "    preds = model.predict_generator(test_data_generator, \n",
    "                                    steps = total_test // batch_size + 1,\n",
    "                                    verbose = 1)\n",
    "    predictions.append(preds)\n",
    "\n",
    "pred = np.mean(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiJm8xxurQAH"
   },
   "outputs": [],
   "source": [
    "pred_pd = pd.DataFrame({'id': test_data_csv.id})\n",
    "\n",
    "test_image_file_list = glob(os.path.join(path, 'image_mask_hsv/test/*.jpg'))\n",
    "pred_id_temp = [os.path.basename(test_image_file_list[i]).replace('.jpg', '') for i in range(total_test)]\n",
    "pred_pd_temp = pd.DataFrame({'id': pred_id_temp})\n",
    "pred_pd_temp['concrete_cement'] = pred[:, 0]\n",
    "pred_pd_temp['healthy_metal'] = pred[:, 1]\n",
    "pred_pd_temp['incomplete'] = pred[:, 2]\n",
    "pred_pd_temp['irregular_metal'] = pred[:, 3]\n",
    "pred_pd_temp['other'] = pred[:, 4]\n",
    "\n",
    "pred_pd = pd.merge(pred_pd, pred_pd_temp, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H23UGWdswFOQ"
   },
   "outputs": [],
   "source": [
    "pred_pd.to_csv(os.path.join(path, 'submission/mixed_inputs_mask_hsv.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPYTy6hy2Fgf"
   },
   "source": [
    "## Prediction for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsB5cizlpct9"
   },
   "outputs": [],
   "source": [
    "datagen_train = ImageDataGenerator(rescale = 1 / 255.,\n",
    "                                  shear_range=0.1,\n",
    "                                  zoom_range=0.1,\n",
    "                                  horizontal_flip=True,\n",
    "                                  vertical_flip=True,\n",
    "                                  rotation_range=10.,\n",
    "                                  width_shift_range = 0.1,\n",
    "                                  height_shift_range = 0.1,\n",
    "                                  featurewise_center = True)\n",
    "datagen_train.mean = [train_data.H_mean.mean() / 255., train_data.S_mean.mean() / 255., train_data.V_mean.mean() / 255.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1215466,
     "status": "ok",
     "timestamp": 1574654653063,
     "user": {
      "displayName": "Mizuhiro Suzuki",
      "photoUrl": "",
      "userId": "17222470123546912393"
     },
     "user_tz": 360
    },
    "id": "dPhY2Kgwf5Qk",
    "outputId": "19b6a71c-1bbd-45cf-be6c-71d804ffbfec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "153/153 [==============================] - 138s 901ms/step\n",
      "1\n",
      "153/153 [==============================] - 137s 894ms/step\n",
      "2\n",
      "153/153 [==============================] - 136s 889ms/step\n",
      "3\n",
      "153/153 [==============================] - 137s 895ms/step\n",
      "4\n",
      "153/153 [==============================] - 146s 954ms/step\n",
      "5\n",
      "153/153 [==============================] - 137s 893ms/step\n",
      "6\n",
      "153/153 [==============================] - 136s 891ms/step\n",
      "7\n",
      "153/153 [==============================] - 136s 892ms/step\n",
      "8\n",
      "153/153 [==============================] - 135s 882ms/step\n",
      "9\n",
      "153/153 [==============================] - 135s 883ms/step\n",
      "10\n",
      "153/153 [==============================] - 134s 879ms/step\n",
      "11\n",
      "153/153 [==============================] - 135s 882ms/step\n",
      "12\n",
      "153/153 [==============================] - 134s 876ms/step\n",
      "13\n",
      "153/153 [==============================] - 133s 868ms/step\n",
      "14\n",
      "153/153 [==============================] - 134s 875ms/step\n",
      "15\n",
      "153/153 [==============================] - 133s 867ms/step\n",
      "16\n",
      "153/153 [==============================] - 134s 879ms/step\n",
      "17\n",
      "153/153 [==============================] - 137s 895ms/step\n",
      "18\n",
      "153/153 [==============================] - 132s 862ms/step\n",
      "19\n",
      "153/153 [==============================] - 132s 863ms/step\n"
     ]
    }
   ],
   "source": [
    "tta_steps = 20\n",
    "predictions = []\n",
    "train_image_file_list = glob(os.path.join(path, 'image_mask_hsv/train/*.jpg'))\n",
    "total_train = len(train_image_file_list)\n",
    "\n",
    "for i in range(tta_steps):\n",
    "    print(i)\n",
    "    train_data_generator = custom_generator(train_image_file_list, train_data_csv, \n",
    "                                          lb_label, continuous_var_list, categorical_var_list, \n",
    "                                          minmaxscaler, lb_list, \n",
    "                                          batch_size, mode = 'eval', augment = datagen_train)\n",
    "    preds = model.predict_generator(train_data_generator, \n",
    "                                    steps = total_train // batch_size + 1,\n",
    "                                    verbose = 1)\n",
    "    predictions.append(preds)\n",
    "\n",
    "pred = np.mean(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tiJm8xxurQAH"
   },
   "outputs": [],
   "source": [
    "pred_pd = pd.DataFrame({'id': train_data_csv.id})\n",
    "\n",
    "train_image_file_list = glob(os.path.join(path, 'image_mask_hsv/train/*.jpg'))\n",
    "pred_id_temp = [os.path.basename(train_image_file_list[i]).replace('.jpg', '') for i in range(total_train)]\n",
    "pred_pd_temp = pd.DataFrame({'id': pred_id_temp})\n",
    "pred_pd_temp['concrete_cement'] = pred[:, 0]\n",
    "pred_pd_temp['healthy_metal'] = pred[:, 1]\n",
    "pred_pd_temp['incomplete'] = pred[:, 2]\n",
    "pred_pd_temp['irregular_metal'] = pred[:, 3]\n",
    "pred_pd_temp['other'] = pred[:, 4]\n",
    "\n",
    "pred_pd = pd.merge(pred_pd, pred_pd_temp, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H23UGWdswFOQ"
   },
   "outputs": [],
   "source": [
    "pred_pd.to_csv(os.path.join(path, 'feature/mixed_inputs_mask_hsv.csv'), index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mixed_inputs_cnn_mask_hsv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
